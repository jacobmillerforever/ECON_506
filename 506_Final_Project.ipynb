{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDiawbY3mVcqG1jwmC8mzj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobmillerforever/ECON_506/blob/main/506_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction & Setup"
      ],
      "metadata": {
        "id": "kegfoQH7dxYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fredapi\n",
        "!pip install investpy"
      ],
      "metadata": {
        "id": "zimnwBagdHqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "from fredapi import Fred\n",
        "import investpy"
      ],
      "metadata": {
        "id": "7IStQ_96dMOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection & Preparation"
      ],
      "metadata": {
        "id": "lenAt87od0vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ticker_data(ticker_dict, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetches data for multiple tickers and creates a DataFrame for each with\n",
        "    single-index columns named as Ticker_ColumnName (e.g., SPY_Close)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_dict : dict\n",
        "        Dictionary with display names as keys and ticker symbols as values\n",
        "    start_date : str\n",
        "        Start date in format 'YYYY-MM-DD'\n",
        "    end_date : str\n",
        "        End date in format 'YYYY-MM-DD'\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with display names as keys and their respective DataFrames as values\n",
        "    \"\"\"\n",
        "    ticker_dataframes = {}\n",
        "\n",
        "    for display_name, ticker_symbol in ticker_dict.items():\n",
        "        # Fetch data for current ticker\n",
        "        data = yf.download(ticker_symbol, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "        # Handle multi-index columns if present\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            # Flatten the multi-index columns to single index\n",
        "            data.columns = [f\"{ticker_symbol}_{col[0]}\" for col in data.columns]\n",
        "        else:\n",
        "            # If not multi-index, still rename columns to match pattern\n",
        "            data.columns = [f\"{ticker_symbol}_{col}\" for col in data.columns]\n",
        "\n",
        "        # Store the DataFrame in the dictionary with display name as key\n",
        "        ticker_dataframes[display_name] = data\n",
        "\n",
        "    return ticker_dataframes\n",
        "\n",
        "tickers = {\n",
        "    # Global Indices\n",
        "    'Nikkei 225 (Japan)': '^N225',\n",
        "    'Hang Seng (Hong Kong)': '^HSI',\n",
        "    'SSE Composite (China)': '000001.SS',\n",
        "    'ASX 200 (Australia)': '^AXJO',\n",
        "    'DAX (Germany)': '^GDAXI',\n",
        "    'FTSE 100 (UK)': '^FTSE',\n",
        "    'CAC 40 (France)': '^FCHI',\n",
        "    'Euro Stoxx 50 (EU)': '^STOXX50E',\n",
        "    'SPY (US)': 'SPY',\n",
        "\n",
        "\n",
        "    # Volatility Indices\n",
        "    'VIX (US)': '^VIX',\n",
        "    'VIX Brazil': '^VXEWZ',\n",
        "    'DAX Volatility': '^VDAX',\n",
        "\n",
        "    # Currency Pairs\n",
        "    'US Dollar Index': 'DX-Y.NYB',\n",
        "    'EUR/USD': 'EURUSD=X',\n",
        "    'JPY/USD': 'JPY=X',\n",
        "    'CNY/USD': 'CNY=X',\n",
        "\n",
        "    # Commodities\n",
        "    'Gold': 'GC=F',\n",
        "    'Crude Oil': 'CL=F',\n",
        "    'Silver': 'SI=F',\n",
        "    'Corn': 'ZC=F',\n",
        "    'Copper': 'HG=F'\n",
        "}\n",
        "\n",
        "start_date = '2000-01-01'\n",
        "end_date = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "# Get individual DataFrames for each ticker\n",
        "ticker_data = get_ticker_data(tickers, start_date, end_date)\n",
        "\n",
        "# Display the first few rows and column names for each DataFrame\n",
        "for display_name, df in ticker_data.items():\n",
        "    print(f\"\\n{display_name} DataFrame:\")\n",
        "    print(f\"Column names: {df.columns.tolist()}\")\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "gl-IDnFda5K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fred_data(api_key, series_list, start_date='2000-01-01', end_date=None):\n",
        "    \"\"\"\n",
        "    Fetches data for multiple FRED series at the highest available frequency\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    api_key : str\n",
        "        Your FRED API key\n",
        "    series_list : list\n",
        "        List of FRED series IDs as strings\n",
        "    start_date : str, optional\n",
        "        Start date in format 'YYYY-MM-DD', defaults to '2000-01-01'\n",
        "    end_date : str, optional\n",
        "        End date in format 'YYYY-MM-DD', defaults to current date\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with series IDs as keys and their respective DataFrames as values\n",
        "    dict\n",
        "        Dictionary with series IDs as keys and the frequency used as values\n",
        "    \"\"\"\n",
        "    # Initialize FRED API connection\n",
        "    fred = Fred(api_key=api_key)\n",
        "\n",
        "    # Set end date to current date if not provided\n",
        "    if end_date is None:\n",
        "        end_date = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Convert start and end dates to datetime objects\n",
        "    start_dt = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end_dt = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "\n",
        "    # Initialize dictionaries to store DataFrames and frequencies\n",
        "    fred_dataframes = {}\n",
        "    fred_frequencies = {}\n",
        "\n",
        "    # Frequency hierarchy from highest to lowest resolution\n",
        "    # Not all series support all frequencies\n",
        "    frequency_hierarchy = ['d', 'w', 'bw', 'm', 'q', 'sa', 'a']\n",
        "\n",
        "    # Process each series ID\n",
        "    for series_id in series_list:\n",
        "        # Try frequencies in order from highest to lowest resolution\n",
        "        for freq in frequency_hierarchy:\n",
        "            try:\n",
        "                # Get data for current series with current frequency\n",
        "                data = fred.get_series(series_id, start_dt, end_dt, frequency=freq)\n",
        "\n",
        "                # If successful and data is not empty, convert to DataFrame\n",
        "                if not data.empty:\n",
        "                    # Convert Series to DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "                    df.columns = [f\"{series_id}_value\"]\n",
        "\n",
        "                    # Add to dictionaries\n",
        "                    fred_dataframes[series_id] = df\n",
        "                    fred_frequencies[series_id] = freq\n",
        "\n",
        "                    print(f\"Successfully fetched data for {series_id} with frequency '{freq}'\")\n",
        "                    # Break out of frequency loop once we've found a working frequency\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"No data found for {series_id} with frequency '{freq}'\")\n",
        "            except Exception as e:\n",
        "                # If this frequency doesn't work, try the next one\n",
        "                print(f\"Could not fetch {series_id} with frequency '{freq}': {str(e)}\")\n",
        "\n",
        "        # Check if we were able to fetch this series with any frequency\n",
        "        if series_id not in fred_dataframes:\n",
        "            print(f\"Failed to fetch data for {series_id} with any available frequency\")\n",
        "\n",
        "    return fred_dataframes, fred_frequencies\n",
        "\n",
        "from google.colab import userdata\n",
        "fred_api = '8b000b950d5841b5b7e35ebbcacedaea'\n",
        "\n",
        "fred_series = [\n",
        "    'DFF',           # Federal Funds Rate\n",
        "    'T10Y2Y',        # 10-Year minus 2-Year Treasury Spread\n",
        "    'CPIAUCSL',      # Consumer Price Index\n",
        "    'UNRATE',        # Unemployment Rate\n",
        "    'STLFSI',        # St. Louis Fed Financial Stress Index\n",
        "    'M2SL',          # M2 Money Supply\n",
        "    'USSLIND',       # US Leading Index\n",
        "    'BAMLH0A0HYM2',  # High Yield Spread\n",
        "    'GS5',           # 5-Year Treasury Rate\n",
        "    'GS30',          # 30-Year Treasury Rate\n",
        "    'BAMLC0A0CM'     # Corporate Bond Spread\n",
        "]\n",
        "\n",
        "fred_data = get_fred_data(fred_api, fred_series)"
      ],
      "metadata": {
        "id": "j3dC6dlDcMT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fred_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy7UUf7W7jz_",
        "outputId": "aeb4bd1b-4b68-4afd-c856-8df6d6d590a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'DFF':             DFF_value\n",
              "  2000-01-01       3.99\n",
              "  2000-01-02       3.99\n",
              "  2000-01-03       5.43\n",
              "  2000-01-04       5.38\n",
              "  2000-01-05       5.41\n",
              "  ...               ...\n",
              "  2025-04-20       4.33\n",
              "  2025-04-21       4.33\n",
              "  2025-04-22       4.33\n",
              "  2025-04-23       4.33\n",
              "  2025-04-24       4.33\n",
              "  \n",
              "  [9246 rows x 1 columns],\n",
              "  'T10Y2Y':             T10Y2Y_value\n",
              "  2000-01-03          0.20\n",
              "  2000-01-04          0.19\n",
              "  2000-01-05          0.24\n",
              "  2000-01-06          0.22\n",
              "  2000-01-07          0.21\n",
              "  ...                  ...\n",
              "  2025-04-21          0.67\n",
              "  2025-04-22          0.65\n",
              "  2025-04-23          0.59\n",
              "  2025-04-24          0.55\n",
              "  2025-04-25          0.55\n",
              "  \n",
              "  [6605 rows x 1 columns],\n",
              "  'CPIAUCSL':             CPIAUCSL_value\n",
              "  2000-01-01         169.300\n",
              "  2000-02-01         170.000\n",
              "  2000-03-01         171.000\n",
              "  2000-04-01         170.900\n",
              "  2000-05-01         171.200\n",
              "  ...                    ...\n",
              "  2024-11-01         316.449\n",
              "  2024-12-01         317.603\n",
              "  2025-01-01         319.086\n",
              "  2025-02-01         319.775\n",
              "  2025-03-01         319.615\n",
              "  \n",
              "  [303 rows x 1 columns],\n",
              "  'UNRATE':             UNRATE_value\n",
              "  2000-01-01           4.0\n",
              "  2000-02-01           4.1\n",
              "  2000-03-01           4.0\n",
              "  2000-04-01           3.8\n",
              "  2000-05-01           4.0\n",
              "  ...                  ...\n",
              "  2024-11-01           4.2\n",
              "  2024-12-01           4.1\n",
              "  2025-01-01           4.0\n",
              "  2025-02-01           4.1\n",
              "  2025-03-01           4.2\n",
              "  \n",
              "  [303 rows x 1 columns],\n",
              "  'STLFSI':             STLFSI_value\n",
              "  2000-01-07         1.095\n",
              "  2000-01-14         1.077\n",
              "  2000-01-21         1.035\n",
              "  2000-01-28         1.008\n",
              "  2000-02-04         1.018\n",
              "  ...                  ...\n",
              "  2020-02-14        -1.604\n",
              "  2020-02-21        -1.596\n",
              "  2020-02-28        -1.215\n",
              "  2020-03-06        -0.942\n",
              "  2020-03-13        -0.070\n",
              "  \n",
              "  [1054 rows x 1 columns],\n",
              "  'M2SL':             M2SL_value\n",
              "  2000-01-01      4667.6\n",
              "  2000-02-01      4680.9\n",
              "  2000-03-01      4711.7\n",
              "  2000-04-01      4767.8\n",
              "  2000-05-01      4755.7\n",
              "  ...                ...\n",
              "  2024-11-01     21454.2\n",
              "  2024-12-01     21489.9\n",
              "  2025-01-01     21577.0\n",
              "  2025-02-01     21670.5\n",
              "  2025-03-01     21762.5\n",
              "  \n",
              "  [303 rows x 1 columns],\n",
              "  'USSLIND':             USSLIND_value\n",
              "  2000-01-01           1.61\n",
              "  2000-02-01           1.48\n",
              "  2000-03-01           1.88\n",
              "  2000-04-01           2.13\n",
              "  2000-05-01           1.85\n",
              "  ...                   ...\n",
              "  2019-10-01           1.41\n",
              "  2019-11-01           1.38\n",
              "  2019-12-01           1.48\n",
              "  2020-01-01           1.57\n",
              "  2020-02-01           1.72\n",
              "  \n",
              "  [242 rows x 1 columns],\n",
              "  'BAMLH0A0HYM2':             BAMLH0A0HYM2_value\n",
              "  2000-01-03                4.68\n",
              "  2000-01-04                4.81\n",
              "  2000-01-05                4.77\n",
              "  2000-01-06                4.82\n",
              "  2000-01-07                4.86\n",
              "  ...                        ...\n",
              "  2025-04-18                 NaN\n",
              "  2025-04-21                4.16\n",
              "  2025-04-22                3.99\n",
              "  2025-04-23                3.75\n",
              "  2025-04-24                3.73\n",
              "  \n",
              "  [6688 rows x 1 columns],\n",
              "  'GS5':             GS5_value\n",
              "  2000-01-01       6.58\n",
              "  2000-02-01       6.68\n",
              "  2000-03-01       6.50\n",
              "  2000-04-01       6.26\n",
              "  2000-05-01       6.69\n",
              "  ...               ...\n",
              "  2024-11-01       4.23\n",
              "  2024-12-01       4.25\n",
              "  2025-01-01       4.43\n",
              "  2025-02-01       4.28\n",
              "  2025-03-01       4.04\n",
              "  \n",
              "  [303 rows x 1 columns],\n",
              "  'GS30':             GS30_value\n",
              "  2000-01-01        6.63\n",
              "  2000-02-01        6.23\n",
              "  2000-03-01        6.05\n",
              "  2000-04-01        5.85\n",
              "  2000-05-01        6.15\n",
              "  ...                ...\n",
              "  2024-11-01        4.54\n",
              "  2024-12-01        4.58\n",
              "  2025-01-01        4.85\n",
              "  2025-02-01        4.68\n",
              "  2025-03-01        4.60\n",
              "  \n",
              "  [303 rows x 1 columns],\n",
              "  'BAMLC0A0CM':             BAMLC0A0CM_value\n",
              "  2000-01-03              1.16\n",
              "  2000-01-04              1.16\n",
              "  2000-01-05              1.18\n",
              "  2000-01-06              1.18\n",
              "  2000-01-07              1.20\n",
              "  ...                      ...\n",
              "  2025-04-18               NaN\n",
              "  2025-04-21              1.12\n",
              "  2025-04-22              1.11\n",
              "  2025-04-23              1.08\n",
              "  2025-04-24              1.06\n",
              "  \n",
              "  [6688 rows x 1 columns]},\n",
              " {'DFF': 'd',\n",
              "  'T10Y2Y': 'd',\n",
              "  'CPIAUCSL': 'm',\n",
              "  'UNRATE': 'm',\n",
              "  'STLFSI': 'w',\n",
              "  'M2SL': 'm',\n",
              "  'USSLIND': 'm',\n",
              "  'BAMLH0A0HYM2': 'd',\n",
              "  'GS5': 'm',\n",
              "  'GS30': 'm',\n",
              "  'BAMLC0A0CM': 'd'})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calendar_df = investpy.economic_calendar(\n",
        "      from_date='01/01/2000',\n",
        "      to_date='31/12/2025',\n",
        "      countries=['united states'],\n",
        "      categories=['monetary policy', 'inflation', 'employment'],\n",
        "      importances=['high']\n",
        ")\n",
        "\n",
        "calendar_df = calendar_df[~calendar_df['importance'].isna()].reset_index(drop=True)\n",
        "calendar_df.tail()\n"
      ],
      "metadata": {
        "id": "2IBARAw-_XD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "9iuXIf8id7Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "def eda_indices_dict(ticker_data_dict):\n",
        "    \"\"\"\n",
        "    Perform EDA on dictionary of DataFrame indices from yfinance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_data_dict : dict\n",
        "        Dictionary with ticker symbols as keys and their DataFrames as values\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Market Indices ===\\n\")\n",
        "\n",
        "    # Summary statistics for each index\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        print(f\"\\n--- {display_name} ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of trading days: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Calculate returns\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        returns = df[close_col].pct_change()\n",
        "\n",
        "        # Summary statistics for close prices\n",
        "        print(f\"\\nClose price statistics:\")\n",
        "        print(f\"Mean: {df[close_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[close_col].std():.2f}\")\n",
        "        print(f\"Min: {df[close_col].min():.2f}\")\n",
        "        print(f\"Max: {df[close_col].max():.2f}\")\n",
        "\n",
        "        # Return statistics\n",
        "        print(f\"\\nDaily return statistics:\")\n",
        "        print(f\"Mean daily return: {returns.mean():.4%}\")\n",
        "        print(f\"Std dev of returns: {returns.std():.4%}\")\n",
        "        print(f\"Sharpe ratio (annualized): {(returns.mean() / returns.std() * np.sqrt(252)):.2f}\")\n",
        "        print(f\"Skewness: {returns.skew():.2f}\")\n",
        "        print(f\"Kurtosis: {returns.kurtosis():.2f}\")\n",
        "\n",
        "        # Plot closing prices and returns\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "        # Price chart\n",
        "        ax1.plot(df.index, df[close_col])\n",
        "        ax1.set_title(f\"{display_name} - Closing Prices\")\n",
        "        ax1.set_ylabel(\"Price\")\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Returns histogram\n",
        "        ax2.hist(returns.dropna(), bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
        "        ax2.set_title(f\"{display_name} - Return Distribution\")\n",
        "        ax2.set_xlabel(\"Daily Returns\")\n",
        "        ax2.set_ylabel(\"Frequency\")\n",
        "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Correlation analysis between indices\n",
        "    print(\"\\n=== Correlation Analysis ===\")\n",
        "    close_prices_dict = {}\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        close_prices_dict[display_name] = df[close_col]\n",
        "\n",
        "    close_prices_df = pd.DataFrame(close_prices_dict)\n",
        "    correlation_matrix = close_prices_df.pct_change().corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix of Daily Returns\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "eda_indices_dict(ticker_data)\n"
      ],
      "metadata": {
        "id": "3frCFPBBH2ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_fred_data(fred_data_tuple):\n",
        "    \"\"\"\n",
        "    Perform EDA on FRED API data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    fred_data_tuple : tuple\n",
        "        Tuple containing (dataframes_dict, frequencies_dict)\n",
        "    \"\"\"\n",
        "    dataframes_dict, frequencies_dict = fred_data_tuple\n",
        "\n",
        "    print(\"=== EDA for FRED Economic Indicators ===\\n\")\n",
        "\n",
        "    # Summary for each FRED series\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        frequency = frequencies_dict[series_id]\n",
        "        print(f\"\\n--- {series_id} (Frequency: {frequency}) ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of observations: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Summary statistics\n",
        "        value_col = df.columns[0]\n",
        "        print(f\"\\nSummary statistics:\")\n",
        "        print(f\"Mean: {df[value_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[value_col].std():.2f}\")\n",
        "        print(f\"Min: {df[value_col].min():.2f}\")\n",
        "        print(f\"Max: {df[value_col].max():.2f}\")\n",
        "\n",
        "        # Calculate percent change based on frequency\n",
        "        if frequency == 'd':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Daily % Change'\n",
        "        elif frequency == 'w':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Weekly % Change'\n",
        "        elif frequency == 'm':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Monthly % Change'\n",
        "        else:\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = '% Change'\n",
        "\n",
        "        # Remove infinite and NaN values\n",
        "        pct_change_clean = pct_change.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(pct_change_clean) > 0:\n",
        "            print(f\"\\n{change_label} statistics:\")\n",
        "            print(f\"Mean: {pct_change_clean.mean():.4%}\")\n",
        "            print(f\"Std Dev: {pct_change_clean.std():.4%}\")\n",
        "\n",
        "            # Plot time series and change distribution\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "            # Time series plot\n",
        "            ax1.plot(df.index, df[value_col])\n",
        "            ax1.set_title(f\"{series_id} - Time Series\")\n",
        "            ax1.set_ylabel(\"Value\")\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Change distribution\n",
        "            try:\n",
        "                ax2.hist(pct_change_clean, bins=50, alpha=0.75, color='green', edgecolor='black')\n",
        "                ax2.set_title(f\"{series_id} - {change_label} Distribution\")\n",
        "                ax2.set_xlabel(change_label)\n",
        "                ax2.set_ylabel(\"Frequency\")\n",
        "                ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: Could not create histogram for {series_id}: {str(e)}\")\n",
        "                ax2.text(0.5, 0.5, 'Histogram not available\\ndue to data issues',\n",
        "                         ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: No valid {change_label} data available for {series_id}\")\n",
        "\n",
        "    # Combine all FRED data for correlation analysis\n",
        "    print(\"\\n=== Cross-Series Analysis ===\")\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        # Resample all series to monthly frequency for comparison\n",
        "        if frequencies_dict[series_id] == 'd':\n",
        "            resampled = df.resample('M').last()\n",
        "        elif frequencies_dict[series_id] == 'w':\n",
        "            resampled = df.resample('M').last()\n",
        "        else:\n",
        "            resampled = df\n",
        "\n",
        "        combined_df[series_id] = resampled[resampled.columns[0]]\n",
        "\n",
        "    # Calculate correlation matrix with handling for NaN values\n",
        "    combined_pct_change = combined_df.pct_change(fill_method=None)\n",
        "    combined_pct_change_clean = combined_pct_change.replace([np.inf, -np.inf], np.nan)\n",
        "    correlation_matrix = combined_pct_change_clean.corr()\n",
        "\n",
        "    if not correlation_matrix.empty:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "        plt.title(\"Correlation Matrix of Economic Indicators (Monthly % Changes)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Warning: Not enough valid data to create correlation matrix\")\n",
        "\n",
        "eda_fred_data((fred_data[0], fred_data[1]))\n"
      ],
      "metadata": {
        "id": "-TMHwNHFH6_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "def eda_indices_dict(ticker_data_dict):\n",
        "    \"\"\"\n",
        "    Perform EDA on dictionary of DataFrame indices from yfinance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_data_dict : dict\n",
        "        Dictionary with ticker symbols as keys and their DataFrames as values\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Market Indices ===\\n\")\n",
        "\n",
        "    # Find the common start date and individual start dates\n",
        "    all_start_dates = {}\n",
        "    all_end_dates = {}\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        all_start_dates[display_name] = df.index.min()\n",
        "        all_end_dates[display_name] = df.index.max()\n",
        "\n",
        "    common_start_date = max(all_start_dates.values())\n",
        "    common_end_date = min(all_end_dates.values())\n",
        "\n",
        "    print(f\"Common data period (all indices available): {common_start_date.date()} to {common_end_date.date()}\")\n",
        "    print(f\"Total common trading days: {sum(1 for d in pd.date_range(common_start_date, common_end_date, freq='B'))}\")\n",
        "\n",
        "    # Summary statistics for each index\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        print(f\"\\n--- {display_name} ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of trading days: {len(df)}\")\n",
        "\n",
        "        # Check data availability\n",
        "        if df.index.min() > pd.Timestamp('2000-01-01'):\n",
        "            print(f\"⚠️ Data starts after 2000: {df.index.min().date()}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Calculate returns\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        returns = df[close_col].pct_change()\n",
        "\n",
        "        # Summary statistics for close prices\n",
        "        print(f\"\\nClose price statistics:\")\n",
        "        print(f\"Mean: {df[close_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[close_col].std():.2f}\")\n",
        "        print(f\"Min: {df[close_col].min():.2f}\")\n",
        "        print(f\"Max: {df[close_col].max():.2f}\")\n",
        "\n",
        "        # Return statistics\n",
        "        print(f\"\\nDaily return statistics:\")\n",
        "        print(f\"Mean daily return: {returns.mean():.4%}\")\n",
        "        print(f\"Std dev of returns: {returns.std():.4%}\")\n",
        "        print(f\"Sharpe ratio (annualized): {(returns.mean() / returns.std() * np.sqrt(252)):.2f}\")\n",
        "        print(f\"Skewness: {returns.skew():.2f}\")\n",
        "        print(f\"Kurtosis: {returns.kurtosis():.2f}\")\n",
        "\n",
        "        # Plot closing prices and returns\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "        # Price chart\n",
        "        ax1.plot(df.index, df[close_col])\n",
        "        ax1.set_title(f\"{display_name} - Closing Prices\")\n",
        "        ax1.set_ylabel(\"Price\")\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Returns histogram\n",
        "        ax2.hist(returns.dropna(), bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
        "        ax2.set_title(f\"{display_name} - Return Distribution\")\n",
        "        ax2.set_xlabel(\"Daily Returns\")\n",
        "        ax2.set_ylabel(\"Frequency\")\n",
        "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Correlation analysis between indices (using common period)\n",
        "    print(\"\\n=== Correlation Analysis ===\")\n",
        "    close_prices_dict = {}\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        close_prices_dict[display_name] = df[close_col]\n",
        "\n",
        "    close_prices_df = pd.DataFrame(close_prices_dict)\n",
        "\n",
        "    # Common period correlation\n",
        "    common_period_df = close_prices_df.loc[common_start_date:common_end_date]\n",
        "    correlation_matrix_common = common_period_df.pct_change().corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix_common, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title(f\"Correlation Matrix of Daily Returns (Common Period: {common_start_date.date()} to {common_end_date.date()})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # All available data correlation (with missing values)\n",
        "    correlation_matrix_all = close_prices_df.pct_change().corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix_all, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix of Daily Returns (All Available Data)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Data availability timeline\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for i, (display_name, df) in enumerate(ticker_data_dict.items()):\n",
        "        plt.barh(i, (df.index.max() - df.index.min()).days,\n",
        "                left=(df.index.min() - pd.Timestamp('2000-01-01')).days,\n",
        "                height=0.6, label=f\"{display_name}: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "\n",
        "    plt.yticks(range(len(ticker_data_dict)), list(ticker_data_dict.keys()))\n",
        "    plt.xlabel(\"Days since 2000-01-01\")\n",
        "    plt.title(\"Data Availability Timeline for Each Index\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def eda_fred_data(fred_data_tuple):\n",
        "    \"\"\"\n",
        "    Perform EDA on FRED API data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    fred_data_tuple : tuple\n",
        "        Tuple containing (dataframes_dict, frequencies_dict)\n",
        "    \"\"\"\n",
        "    dataframes_dict, frequencies_dict = fred_data_tuple\n",
        "\n",
        "    print(\"=== EDA for FRED Economic Indicators ===\\n\")\n",
        "\n",
        "    # Summary for each FRED series\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        frequency = frequencies_dict[series_id]\n",
        "        print(f\"\\n--- {series_id} (Frequency: {frequency}) ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of observations: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Summary statistics\n",
        "        value_col = df.columns[0]\n",
        "        print(f\"\\nSummary statistics:\")\n",
        "        print(f\"Mean: {df[value_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[value_col].std():.2f}\")\n",
        "        print(f\"Min: {df[value_col].min():.2f}\")\n",
        "        print(f\"Max: {df[value_col].max():.2f}\")\n",
        "\n",
        "        # Calculate percent change based on frequency\n",
        "        if frequency == 'd':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Daily % Change'\n",
        "        elif frequency == 'w':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Weekly % Change'\n",
        "        elif frequency == 'm':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Monthly % Change'\n",
        "        else:\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = '% Change'\n",
        "\n",
        "        # Remove infinite and NaN values\n",
        "        pct_change_clean = pct_change.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(pct_change_clean) > 0:\n",
        "            print(f\"\\n{change_label} statistics:\")\n",
        "            print(f\"Mean: {pct_change_clean.mean():.4%}\")\n",
        "            print(f\"Std Dev: {pct_change_clean.std():.4%}\")\n",
        "\n",
        "            # Plot time series and change distribution\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "            # Time series plot\n",
        "            ax1.plot(df.index, df[value_col])\n",
        "            ax1.set_title(f\"{series_id} - Time Series\")\n",
        "            ax1.set_ylabel(\"Value\")\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Change distribution\n",
        "            try:\n",
        "                ax2.hist(pct_change_clean, bins=50, alpha=0.75, color='green', edgecolor='black')\n",
        "                ax2.set_title(f\"{series_id} - {change_label} Distribution\")\n",
        "                ax2.set_xlabel(change_label)\n",
        "                ax2.set_ylabel(\"Frequency\")\n",
        "                ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: Could not create histogram for {series_id}: {str(e)}\")\n",
        "                ax2.text(0.5, 0.5, 'Histogram not available\\ndue to data issues',\n",
        "                         ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: No valid {change_label} data available for {series_id}\")\n",
        "\n",
        "    # Combine all FRED data for correlation analysis\n",
        "    print(\"\\n=== Cross-Series Analysis ===\")\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        # Resample all series to monthly frequency for comparison\n",
        "        if frequencies_dict[series_id] == 'd':\n",
        "            resampled = df.resample('M').last()\n",
        "        elif frequencies_dict[series_id] == 'w':\n",
        "            resampled = df.resample('M').last()\n",
        "        else:\n",
        "            resampled = df\n",
        "\n",
        "        combined_df[series_id] = resampled[resampled.columns[0]]\n",
        "\n",
        "    # Calculate correlation matrix with handling for NaN values\n",
        "    combined_pct_change = combined_df.pct_change(fill_method=None)\n",
        "    combined_pct_change_clean = combined_pct_change.replace([np.inf, -np.inf], np.nan)\n",
        "    correlation_matrix = combined_pct_change_clean.corr()\n",
        "\n",
        "    if not correlation_matrix.empty:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "        plt.title(\"Correlation Matrix of Economic Indicators (Monthly % Changes)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Warning: Not enough valid data to create correlation matrix\")\n",
        "\n",
        "def eda_calendar_data(calendar_df):\n",
        "    \"\"\"\n",
        "    Perform EDA on economic calendar data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    calendar_df : pandas.DataFrame\n",
        "        DataFrame containing economic calendar data\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Economic Calendar ===\\n\")\n",
        "\n",
        "    # Basic info\n",
        "    print(f\"Date range: {calendar_df['date'].min()} to {calendar_df['date'].max()}\")\n",
        "    print(f\"Total number of events: {len(calendar_df)}\")\n",
        "\n",
        "    # Convert date column to datetime - handle potential type issues\n",
        "    if calendar_df['date'].dtype != 'datetime64[ns]':\n",
        "        try:\n",
        "            # Try converting to string first if necessary\n",
        "            calendar_df['date'] = calendar_df['date'].astype(str)\n",
        "            calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%d/%m/%Y')\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert date column: {e}\")\n",
        "            # Try alternative conversion\n",
        "            try:\n",
        "                calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
        "            except Exception as e2:\n",
        "                print(f\"Error: Unable to convert date column: {e2}\")\n",
        "                return\n",
        "\n",
        "    # Extract year and month for analysis\n",
        "    calendar_df['year'] = calendar_df['date'].dt.year\n",
        "    calendar_df['month'] = calendar_df['date'].dt.month\n",
        "    calendar_df['weekday'] = calendar_df['date'].dt.dayofweek\n",
        "\n",
        "    # Events by type\n",
        "    print(\"\\n--- Event Categories ---\")\n",
        "    event_types = calendar_df['event'].str.extract(r'(.+?)\\s*(?:\\(|\\s*$)')[0].value_counts()\n",
        "    print(event_types.head(15))\n",
        "\n",
        "    # Events by year\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    yearly_events = calendar_df.groupby('year').size()\n",
        "    yearly_events.plot(kind='bar', alpha=0.75, color='blue', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Year\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Events by month\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    monthly_events = calendar_df.groupby('month').size()\n",
        "    monthly_events.plot(kind='bar', alpha=0.75, color='green', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Month\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Events by weekday\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    weekday_events = calendar_df.groupby('weekday').size()\n",
        "    weekday_events.plot(kind='bar', alpha=0.75, color='orange', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Weekday\")\n",
        "    plt.xlabel(\"Weekday\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Time of day analysis\n",
        "    try:\n",
        "        calendar_df['hour'] = pd.to_datetime(calendar_df['time'].astype(str), format='%H:%M').dt.hour\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        hourly_events = calendar_df.groupby('hour').size()\n",
        "        hourly_events.plot(kind='bar', alpha=0.75, color='purple', edgecolor='black')\n",
        "        plt.title(\"Number of Economic Events by Hour of Day\")\n",
        "        plt.xlabel(\"Hour\")\n",
        "        plt.ylabel(\"Number of Events\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not analyze time of day: {e}\")\n",
        "\n",
        "    # # Event importance\n",
        "    # print(\"\\n--- Event Importance ---\")\n",
        "    # importance_counts = calendar_df['importance'].value_counts()\n",
        "    # print(importance_counts)\n",
        "\n",
        "    # Create a heatmap of events by month and year\n",
        "    pivot_table = calendar_df.pivot_table(\n",
        "        values='id',\n",
        "        index='year',\n",
        "        columns='month',\n",
        "        aggfunc='count',\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(pivot_table, cmap='YlOrRd', annot=True, fmt='d')\n",
        "    plt.title(\"Event Count Heatmap by Year and Month\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Year\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Most common event types over time\n",
        "    calendar_df['event_type'] = calendar_df['event'].str.extract(r'(.+?)\\s*(?:\\(|\\s*$)')[0]\n",
        "    top_5_events = event_types.head(5).index\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for event in top_5_events:\n",
        "        event_data = calendar_df[calendar_df['event_type'] == event]\n",
        "        event_by_year = event_data.groupby('year').size()\n",
        "        plt.plot(event_by_year.index, event_by_year.values, marker='o', label=event)\n",
        "\n",
        "    plt.title(\"Top 5 Economic Event Types by Year\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "eda_calendar_data(calendar_df)"
      ],
      "metadata": {
        "id": "0PBXNe6aNQi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "KlRIHTfOeEWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's access the SPY data from your ticker_data dictionary\n",
        "spy_df = ticker_data['SPY (US)'].copy()\n",
        "\n",
        "# Filter to start from the first full market week of 2007\n",
        "# The first trading day of 2007 was January 3rd (Wednesday)\n",
        "# So the first full market week started on January 8th (Monday)\n",
        "start_date = '2007-01-08'\n",
        "spy_df_filtered = spy_df[spy_df.index >= start_date]\n",
        "\n",
        "# Calculate the percent change from open to close\n",
        "spy_df_filtered['pct_change_open_close'] = (spy_df_filtered['SPY_Close'] - spy_df_filtered['SPY_Open']) / spy_df_filtered['SPY_Open'] * 100\n",
        "\n",
        "# Create the target variable trend_i\n",
        "# trend_i = 1 if absolute percent change > 0.5%, else 0\n",
        "spy_df_filtered['trend_i'] = np.where(np.abs(spy_df_filtered['pct_change_open_close']) > 0.5, 1, 0)\n",
        "\n",
        "# Display some summary statistics\n",
        "print(f\"Date range: {spy_df_filtered.index.min().date()} to {spy_df_filtered.index.max().date()}\")\n",
        "print(f\"Total trading days: {len(spy_df_filtered)}\")\n",
        "print(f\"Days with trend (trend_i = 1): {spy_df_filtered['trend_i'].sum()}\")\n",
        "print(f\"Days without trend (trend_i = 0): {len(spy_df_filtered) - spy_df_filtered['trend_i'].sum()}\")\n",
        "print(f\"Percentage of trending days: {spy_df_filtered['trend_i'].mean() * 100:.2f}%\")\n",
        "\n",
        "# Let's create a visualization to understand the distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the distribution of daily percentage changes\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(spy_df_filtered['pct_change_open_close'], bins=100, alpha=0.75, edgecolor='black')\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='0.5% threshold')\n",
        "plt.axvline(x=-0.5, color='red', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Daily % Change (Open to Close)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of SPY Daily Percentage Changes (2007-Present)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a small sample output to verify the calculation\n",
        "print(\"\\nSample of the data with trend_i:\")\n",
        "sample_data = spy_df_filtered[['SPY_Open', 'SPY_Close', 'pct_change_open_close', 'trend_i']].copy()\n",
        "sample_data['abs_pct_change'] = np.abs(sample_data['pct_change_open_close'])\n",
        "print(sample_data.head(10))\n",
        "\n",
        "# Let's also check for balance between trending up and trending down\n",
        "trending_days = spy_df_filtered[spy_df_filtered['trend_i'] == 1]\n",
        "trend_up = trending_days[trending_days['pct_change_open_close'] > 0]\n",
        "trend_down = trending_days[trending_days['pct_change_open_close'] < 0]\n",
        "\n",
        "print(f\"\\nTrending days (|change| > 0.5%): {len(trending_days)}\")\n",
        "print(f\"  Upward trends (change > 0.5%): {len(trend_up)}\")\n",
        "print(f\"  Downward trends (change < -0.5%): {len(trend_down)}\")\n",
        "\n",
        "# Let's also check for any outliers or extreme values\n",
        "print(f\"\\nMaximum daily % change: {spy_df_filtered['pct_change_open_close'].max():.2f}%\")\n",
        "print(f\"Minimum daily % change: {spy_df_filtered['pct_change_open_close'].min():.2f}%\")\n",
        "\n",
        "# Create your base DataFrame for feature engineering\n",
        "base_df = spy_df_filtered.copy()\n",
        "print(f\"\\nBase DataFrame shape: {base_df.shape}\")\n",
        "print(f\"Columns: {base_df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "mQfr6ozmcoPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's create a list of indices to include (excluding VIX Brazil)\n",
        "indices_to_include = [key for key in ticker_data.keys() if key != 'VIX Brazil']\n",
        "\n",
        "# Let's create a function to extract and rename the relevant columns\n",
        "def extract_columns(df, display_name):\n",
        "    # Extract ticker symbol from the column names\n",
        "    ticker_symbol = df.columns[0].split('_')[0]\n",
        "\n",
        "    # Extract relevant columns and rename them\n",
        "    columns_to_extract = {}\n",
        "\n",
        "    if f'{ticker_symbol}_Open' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Open'] = f'{display_name}_Open'\n",
        "    if f'{ticker_symbol}_Close' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Close'] = f'{display_name}_Close'\n",
        "    if f'{ticker_symbol}_Volume' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Volume'] = f'{display_name}_Volume'\n",
        "\n",
        "    # Create a new dataframe with only the relevant columns\n",
        "    extracted_df = df[list(columns_to_extract.keys())].copy()\n",
        "    extracted_df = extracted_df.rename(columns=columns_to_extract)\n",
        "\n",
        "    return extracted_df\n",
        "\n",
        "# Join the data from other indices to the base dataframe\n",
        "for display_name in indices_to_include:\n",
        "    if display_name != 'SPY (US)':  # We already have SPY in the base_df\n",
        "        index_df = ticker_data[display_name]\n",
        "\n",
        "        # Filter to match the date range of base_df\n",
        "        index_df_filtered = index_df[index_df.index >= start_date]\n",
        "\n",
        "        # Extract the relevant columns\n",
        "        extracted_df = extract_columns(index_df_filtered, display_name)\n",
        "\n",
        "        # Join to base_df\n",
        "        base_df = base_df.join(extracted_df, how='left')\n",
        "\n",
        "# Display the resulting dataframe structure\n",
        "print(f\"Base DataFrame shape after joining indices: {base_df.shape}\")\n",
        "print(f\"\\nColumns in base_df:\")\n",
        "for col in base_df.columns:\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "# Check for missing values in the joined data\n",
        "missing_summary = base_df.isnull().sum()\n",
        "if missing_summary.any():\n",
        "    print(\"\\nMissing values in joined data:\")\n",
        "    print(missing_summary[missing_summary > 0])\n",
        "\n",
        "# Sample of the data to verify the join\n",
        "print(\"\\nSample of the joined data:\")\n",
        "sample_columns = ['SPY_Open', 'SPY_Close', 'pct_change_open_close', 'trend_i']\n",
        "# Add some other index columns to the sample\n",
        "for display_name in indices_to_include[:3]:  # Show first 3 indices as example\n",
        "    if display_name != 'SPY (US)':\n",
        "        open_col = f'{display_name}_Open'\n",
        "        if open_col in base_df.columns:\n",
        "            sample_columns.append(open_col)\n",
        "\n",
        "print(base_df[sample_columns].head())\n",
        "\n",
        "# Summary of data availability for each index\n",
        "print(\"\\nData availability summary:\")\n",
        "for display_name in indices_to_include:\n",
        "    if display_name != 'SPY (US)':\n",
        "        open_col = f'{display_name}_Open'\n",
        "        if open_col in base_df.columns:\n",
        "            non_null_count = base_df[open_col].count()\n",
        "            total_rows = len(base_df)\n",
        "            coverage = (non_null_count / total_rows) * 100\n",
        "            print(f\"{display_name}: {non_null_count}/{total_rows} ({coverage:.1f}% coverage)\")"
      ],
      "metadata": {
        "id": "UyJ9eUyKemft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original base_df\n",
        "base_df_10am = base_df.copy()\n",
        "\n",
        "# Define market groups\n",
        "asian_markets = ['Nikkei 225 (Japan)', 'Hang Seng (Hong Kong)', 'SSE Composite (China)', 'ASX 200 (Australia)']\n",
        "european_markets = ['DAX (Germany)', 'FTSE 100 (UK)', 'CAC 40 (France)', 'Euro Stoxx 50 (EU)']\n",
        "us_markets = ['SPY']\n",
        "currency_pairs = ['EUR/USD', 'JPY/USD', 'CNY/USD']\n",
        "commodities = ['Gold', 'Crude Oil', 'Silver', 'Corn', 'Copper']\n",
        "volatility_indices = ['VIX (US)',  'US Dollar Index']\n",
        "\n",
        "# First, create lagged versions of ALL columns\n",
        "for col in base_df.columns:\n",
        "    base_df_10am[f'{col}_lag1'] = base_df[col].shift(1)\n",
        "\n",
        "# Now, create current day columns for specific markets\n",
        "# Asian markets - current day Open, Close, Volume\n",
        "for market in asian_markets:\n",
        "    for col_type in ['Open', 'Close', 'Volume']:\n",
        "        col_name = f'{market}_{col_type}'\n",
        "        if col_name in base_df.columns:\n",
        "            base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# European markets - current day Open only\n",
        "for market in european_markets:\n",
        "    col_name = f'{market}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# SPY - current day Open\n",
        "if 'SPY_Open' in base_df.columns:\n",
        "    base_df_10am['SPY_Open_current'] = base_df['SPY_Open']\n",
        "\n",
        "# Volatility indices - current day Open\n",
        "for market in volatility_indices:\n",
        "    col_name = f'{market}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# Currency pairs - current day Open\n",
        "for pair in currency_pairs:\n",
        "    col_name = f'{pair}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# Commodities - current day Open\n",
        "for commodity in commodities:\n",
        "    col_name = f'{commodity}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# Target variable - current day\n",
        "if 'trend_i' in base_df.columns:\n",
        "    base_df_10am['trend_i_current'] = base_df['trend_i']\n",
        "\n",
        "# Now let's select only the columns we want to keep\n",
        "columns_to_keep = []\n",
        "\n",
        "# Keep lagged versions of everything\n",
        "for col in base_df.columns:\n",
        "    columns_to_keep.append(f'{col}_lag1')\n",
        "\n",
        "# Keep current day Asian Open, Close, Volume\n",
        "for market in asian_markets:\n",
        "    for col_type in ['Open', 'Close', 'Volume']:\n",
        "        col_name = f'{market}_{col_type}_current'\n",
        "        if col_name in base_df_10am.columns:\n",
        "            columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day European Open\n",
        "for market in european_markets:\n",
        "    col_name = f'{market}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day SPY Open\n",
        "if 'SPY_Open_current' in base_df_10am.columns:\n",
        "    columns_to_keep.append('SPY_Open_current')\n",
        "\n",
        "# Keep current day Volatility indices Open\n",
        "for market in volatility_indices:\n",
        "    col_name = f'{market}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day Currency pairs Open\n",
        "for pair in currency_pairs:\n",
        "    col_name = f'{pair}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day Commodities Open\n",
        "for commodity in commodities:\n",
        "    col_name = f'{commodity}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day target variable\n",
        "if 'trend_i_current' in base_df_10am.columns:\n",
        "    columns_to_keep.append('trend_i_current')\n",
        "\n",
        "# Filter to keep only the columns we want\n",
        "base_df_10am = base_df_10am[columns_to_keep]\n",
        "\n",
        "# Drop the first row since it will have NaN values from lagging\n",
        "base_df_10am = base_df_10am.iloc[1:].copy()\n",
        "\n",
        "# Display the structure\n",
        "print(f\"Final dataframe shape: {base_df_10am.shape}\")\n",
        "print(\"\\nColumns in final dataframe:\")\n",
        "for i, col in enumerate(base_df_10am.columns):\n",
        "    print(f\"{i+1:3d}. {col}\")\n",
        "\n",
        "# Verify our structure\n",
        "print(\"\\n=== Data Structure at 10 AM EST ===\")\n",
        "print(\"\\nLagged columns (previous day):\")\n",
        "lagged_cols = [col for col in base_df_10am.columns if '_lag1' in col]\n",
        "print(f\"  Count: {len(lagged_cols)}\")\n",
        "\n",
        "print(\"\\nCurrent day columns:\")\n",
        "current_cols = [col for col in base_df_10am.columns if '_current' in col]\n",
        "print(f\"  Count: {len(current_cols)}\")\n",
        "print(f\"  Open prices: {len([col for col in current_cols if 'Open' in col])}\")\n",
        "print(f\"  Close prices: {len([col for col in current_cols if 'Close' in col])}\")\n",
        "print(f\"  Volume: {len([col for col in current_cols if 'Volume' in col])}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_summary = base_df_10am.isnull().sum()\n",
        "if missing_summary.any():\n",
        "    print(\"\\nMissing values in final dataframe:\")\n",
        "    missing_df = pd.DataFrame({'Missing_Count': missing_summary[missing_summary > 0]})\n",
        "    missing_df['Percentage'] = (missing_df['Missing_Count'] / len(base_df_10am) * 100).round(2)\n",
        "    print(missing_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iCXdfrS3k9V",
        "outputId": "4dc7b21c-9732-4558-b6df-082828bb5163"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final dataframe shape: (4604, 92)\n",
            "\n",
            "Columns in final dataframe:\n",
            "  1. SPY_Close_lag1\n",
            "  2. SPY_High_lag1\n",
            "  3. SPY_Low_lag1\n",
            "  4. SPY_Open_lag1\n",
            "  5. SPY_Volume_lag1\n",
            "  6. pct_change_open_close_lag1\n",
            "  7. trend_i_lag1\n",
            "  8. Nikkei 225 (Japan)_Open_lag1\n",
            "  9. Nikkei 225 (Japan)_Close_lag1\n",
            " 10. Nikkei 225 (Japan)_Volume_lag1\n",
            " 11. Hang Seng (Hong Kong)_Open_lag1\n",
            " 12. Hang Seng (Hong Kong)_Close_lag1\n",
            " 13. Hang Seng (Hong Kong)_Volume_lag1\n",
            " 14. SSE Composite (China)_Open_lag1\n",
            " 15. SSE Composite (China)_Close_lag1\n",
            " 16. SSE Composite (China)_Volume_lag1\n",
            " 17. ASX 200 (Australia)_Open_lag1\n",
            " 18. ASX 200 (Australia)_Close_lag1\n",
            " 19. ASX 200 (Australia)_Volume_lag1\n",
            " 20. DAX (Germany)_Open_lag1\n",
            " 21. DAX (Germany)_Close_lag1\n",
            " 22. DAX (Germany)_Volume_lag1\n",
            " 23. FTSE 100 (UK)_Open_lag1\n",
            " 24. FTSE 100 (UK)_Close_lag1\n",
            " 25. FTSE 100 (UK)_Volume_lag1\n",
            " 26. CAC 40 (France)_Open_lag1\n",
            " 27. CAC 40 (France)_Close_lag1\n",
            " 28. CAC 40 (France)_Volume_lag1\n",
            " 29. Euro Stoxx 50 (EU)_Open_lag1\n",
            " 30. Euro Stoxx 50 (EU)_Close_lag1\n",
            " 31. Euro Stoxx 50 (EU)_Volume_lag1\n",
            " 32. VIX (US)_Open_lag1\n",
            " 33. VIX (US)_Close_lag1\n",
            " 34. VIX (US)_Volume_lag1\n",
            " 35. DAX Volatility_Open_lag1\n",
            " 36. DAX Volatility_Close_lag1\n",
            " 37. DAX Volatility_Volume_lag1\n",
            " 38. US Dollar Index_Open_lag1\n",
            " 39. US Dollar Index_Close_lag1\n",
            " 40. US Dollar Index_Volume_lag1\n",
            " 41. EUR/USD_Open_lag1\n",
            " 42. EUR/USD_Close_lag1\n",
            " 43. EUR/USD_Volume_lag1\n",
            " 44. JPY/USD_Open_lag1\n",
            " 45. JPY/USD_Close_lag1\n",
            " 46. JPY/USD_Volume_lag1\n",
            " 47. CNY/USD_Open_lag1\n",
            " 48. CNY/USD_Close_lag1\n",
            " 49. CNY/USD_Volume_lag1\n",
            " 50. Gold_Open_lag1\n",
            " 51. Gold_Close_lag1\n",
            " 52. Gold_Volume_lag1\n",
            " 53. Crude Oil_Open_lag1\n",
            " 54. Crude Oil_Close_lag1\n",
            " 55. Crude Oil_Volume_lag1\n",
            " 56. Silver_Open_lag1\n",
            " 57. Silver_Close_lag1\n",
            " 58. Silver_Volume_lag1\n",
            " 59. Corn_Open_lag1\n",
            " 60. Corn_Close_lag1\n",
            " 61. Corn_Volume_lag1\n",
            " 62. Copper_Open_lag1\n",
            " 63. Copper_Close_lag1\n",
            " 64. Copper_Volume_lag1\n",
            " 65. Nikkei 225 (Japan)_Open_current\n",
            " 66. Nikkei 225 (Japan)_Close_current\n",
            " 67. Nikkei 225 (Japan)_Volume_current\n",
            " 68. Hang Seng (Hong Kong)_Open_current\n",
            " 69. Hang Seng (Hong Kong)_Close_current\n",
            " 70. Hang Seng (Hong Kong)_Volume_current\n",
            " 71. SSE Composite (China)_Open_current\n",
            " 72. SSE Composite (China)_Close_current\n",
            " 73. SSE Composite (China)_Volume_current\n",
            " 74. ASX 200 (Australia)_Open_current\n",
            " 75. ASX 200 (Australia)_Close_current\n",
            " 76. ASX 200 (Australia)_Volume_current\n",
            " 77. DAX (Germany)_Open_current\n",
            " 78. FTSE 100 (UK)_Open_current\n",
            " 79. CAC 40 (France)_Open_current\n",
            " 80. Euro Stoxx 50 (EU)_Open_current\n",
            " 81. SPY_Open_current\n",
            " 82. VIX (US)_Open_current\n",
            " 83. US Dollar Index_Open_current\n",
            " 84. EUR/USD_Open_current\n",
            " 85. JPY/USD_Open_current\n",
            " 86. CNY/USD_Open_current\n",
            " 87. Gold_Open_current\n",
            " 88. Crude Oil_Open_current\n",
            " 89. Silver_Open_current\n",
            " 90. Corn_Open_current\n",
            " 91. Copper_Open_current\n",
            " 92. trend_i_current\n",
            "\n",
            "=== Data Structure at 10 AM EST ===\n",
            "\n",
            "Lagged columns (previous day):\n",
            "  Count: 64\n",
            "\n",
            "Current day columns:\n",
            "  Count: 28\n",
            "  Open prices: 19\n",
            "  Close prices: 4\n",
            "  Volume: 4\n",
            "\n",
            "Missing values in final dataframe:\n",
            "                                  Missing_Count  Percentage\n",
            "Nikkei 225 (Japan)_Open_lag1                281        6.10\n",
            "Nikkei 225 (Japan)_Close_lag1               281        6.10\n",
            "Nikkei 225 (Japan)_Volume_lag1              281        6.10\n",
            "Hang Seng (Hong Kong)_Open_lag1             215        4.67\n",
            "Hang Seng (Hong Kong)_Close_lag1            215        4.67\n",
            "...                                         ...         ...\n",
            "Gold_Open_current                             3        0.07\n",
            "Crude Oil_Open_current                        2        0.04\n",
            "Silver_Open_current                           3        0.07\n",
            "Corn_Open_current                             4        0.09\n",
            "Copper_Open_current                           3        0.07\n",
            "\n",
            "[79 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development"
      ],
      "metadata": {
        "id": "6l5IjaqLeHHg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuUg-EHj80o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "4ug4MBM5eN85"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dExarGOxd6QP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}