{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQfShpVQqFu8XVTXRX0WZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobmillerforever/ECON_506/blob/main/506_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction & Setup"
      ],
      "metadata": {
        "id": "kegfoQH7dxYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fredapi\n",
        "!pip install investpy\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "zimnwBagdHqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(f\"GPUs available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")"
      ],
      "metadata": {
        "id": "NidJ2g_MCRlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "from fredapi import Fred\n",
        "import investpy\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, MaxPooling1D, GlobalAveragePooling1D, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "7IStQ_96dMOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection & Preparation"
      ],
      "metadata": {
        "id": "lenAt87od0vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ticker_data(ticker_dict, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetches data for multiple tickers and creates a DataFrame for each with\n",
        "    single-index columns named as Ticker_ColumnName (e.g., SPY_Close)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_dict : dict\n",
        "        Dictionary with display names as keys and ticker symbols as values\n",
        "    start_date : str\n",
        "        Start date in format 'YYYY-MM-DD'\n",
        "    end_date : str\n",
        "        End date in format 'YYYY-MM-DD'\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with display names as keys and their respective DataFrames as values\n",
        "    \"\"\"\n",
        "    ticker_dataframes = {}\n",
        "\n",
        "    for display_name, ticker_symbol in ticker_dict.items():\n",
        "        # Fetch data for current ticker\n",
        "        data = yf.download(ticker_symbol, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "        # Handle multi-index columns if present\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            # Flatten the multi-index columns to single index\n",
        "            data.columns = [f\"{ticker_symbol}_{col[0]}\" for col in data.columns]\n",
        "        else:\n",
        "            # If not multi-index, still rename columns to match pattern\n",
        "            data.columns = [f\"{ticker_symbol}_{col}\" for col in data.columns]\n",
        "\n",
        "        # Store the DataFrame in the dictionary with display name as key\n",
        "        ticker_dataframes[display_name] = data\n",
        "\n",
        "    return ticker_dataframes\n",
        "\n",
        "tickers = {\n",
        "    # Global Indices\n",
        "    'Nikkei 225 (Japan)': '^N225',\n",
        "    'Hang Seng (Hong Kong)': '^HSI',\n",
        "    'SSE Composite (China)': '000001.SS',\n",
        "    'ASX 200 (Australia)': '^AXJO',\n",
        "    'DAX (Germany)': '^GDAXI',\n",
        "    'FTSE 100 (UK)': '^FTSE',\n",
        "    'CAC 40 (France)': '^FCHI',\n",
        "    'Euro Stoxx 50 (EU)': '^STOXX50E',\n",
        "    'SPY (US)': 'SPY',\n",
        "\n",
        "\n",
        "    # Volatility Indices\n",
        "    'VIX (US)': '^VIX',\n",
        "    'VIX Brazil': '^VXEWZ',\n",
        "    'DAX Volatility': '^VDAX',\n",
        "\n",
        "    # Currency Pairs\n",
        "    'US Dollar Index': 'DX-Y.NYB',\n",
        "    'EUR/USD': 'EURUSD=X',\n",
        "    'JPY/USD': 'JPY=X',\n",
        "    'CNY/USD': 'CNY=X',\n",
        "\n",
        "    # Commodities\n",
        "    'Gold': 'GC=F',\n",
        "    'Crude Oil': 'CL=F',\n",
        "    'Silver': 'SI=F',\n",
        "    'Corn': 'ZC=F',\n",
        "    'Copper': 'HG=F'\n",
        "}\n",
        "\n",
        "start_date = '2000-01-01'\n",
        "end_date = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "# Get individual DataFrames for each ticker\n",
        "ticker_data = get_ticker_data(tickers, start_date, end_date)\n",
        "\n",
        "# Display the first few rows and column names for each DataFrame\n",
        "for display_name, df in ticker_data.items():\n",
        "    print(f\"\\n{display_name} DataFrame:\")\n",
        "    print(f\"Column names: {df.columns.tolist()}\")\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "gl-IDnFda5K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fred_data(api_key, series_list, start_date='2000-01-01', end_date=None):\n",
        "    \"\"\"\n",
        "    Fetches data for multiple FRED series at the highest available frequency\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    api_key : str\n",
        "        Your FRED API key\n",
        "    series_list : list\n",
        "        List of FRED series IDs as strings\n",
        "    start_date : str, optional\n",
        "        Start date in format 'YYYY-MM-DD', defaults to '2000-01-01'\n",
        "    end_date : str, optional\n",
        "        End date in format 'YYYY-MM-DD', defaults to current date\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with series IDs as keys and their respective DataFrames as values\n",
        "    dict\n",
        "        Dictionary with series IDs as keys and the frequency used as values\n",
        "    \"\"\"\n",
        "    # Initialize FRED API connection\n",
        "    fred = Fred(api_key=api_key)\n",
        "\n",
        "    # Set end date to current date if not provided\n",
        "    if end_date is None:\n",
        "        end_date = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Convert start and end dates to datetime objects\n",
        "    start_dt = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end_dt = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "\n",
        "    # Initialize dictionaries to store DataFrames and frequencies\n",
        "    fred_dataframes = {}\n",
        "    fred_frequencies = {}\n",
        "\n",
        "    # Frequency hierarchy from highest to lowest resolution\n",
        "    # Not all series support all frequencies\n",
        "    frequency_hierarchy = ['d', 'w', 'bw', 'm', 'q', 'sa', 'a']\n",
        "\n",
        "    # Process each series ID\n",
        "    for series_id in series_list:\n",
        "        # Try frequencies in order from highest to lowest resolution\n",
        "        for freq in frequency_hierarchy:\n",
        "            try:\n",
        "                # Get data for current series with current frequency\n",
        "                data = fred.get_series(series_id, start_dt, end_dt, frequency=freq)\n",
        "\n",
        "                # If successful and data is not empty, convert to DataFrame\n",
        "                if not data.empty:\n",
        "                    # Convert Series to DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "                    df.columns = [f\"{series_id}_value\"]\n",
        "\n",
        "                    # Add to dictionaries\n",
        "                    fred_dataframes[series_id] = df\n",
        "                    fred_frequencies[series_id] = freq\n",
        "\n",
        "                    print(f\"Successfully fetched data for {series_id} with frequency '{freq}'\")\n",
        "                    # Break out of frequency loop once we've found a working frequency\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"No data found for {series_id} with frequency '{freq}'\")\n",
        "            except Exception as e:\n",
        "                # If this frequency doesn't work, try the next one\n",
        "                print(f\"Could not fetch {series_id} with frequency '{freq}': {str(e)}\")\n",
        "\n",
        "        # Check if we were able to fetch this series with any frequency\n",
        "        if series_id not in fred_dataframes:\n",
        "            print(f\"Failed to fetch data for {series_id} with any available frequency\")\n",
        "\n",
        "    return fred_dataframes, fred_frequencies\n",
        "\n",
        "from google.colab import userdata\n",
        "fred_api = '8b000b950d5841b5b7e35ebbcacedaea'\n",
        "\n",
        "fred_series = [\n",
        "    'DFF',           # Federal Funds Rate\n",
        "    'T10Y2Y',        # 10-Year minus 2-Year Treasury Spread\n",
        "    'CPIAUCSL',      # Consumer Price Index\n",
        "    'UNRATE',        # Unemployment Rate\n",
        "    'STLFSI',        # St. Louis Fed Financial Stress Index\n",
        "    'M2SL',          # M2 Money Supply\n",
        "    'USSLIND',       # US Leading Index\n",
        "    'BAMLH0A0HYM2',  # High Yield Spread\n",
        "    'GS5',           # 5-Year Treasury Rate\n",
        "    'GS30',          # 30-Year Treasury Rate\n",
        "    'BAMLC0A0CM'     # Corporate Bond Spread\n",
        "]\n",
        "\n",
        "fred_data = get_fred_data(fred_api, fred_series)"
      ],
      "metadata": {
        "id": "j3dC6dlDcMT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fred_data"
      ],
      "metadata": {
        "id": "wy7UUf7W7jz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calendar_df = investpy.economic_calendar(\n",
        "      from_date='01/01/2000',\n",
        "      to_date='31/12/2025',\n",
        "      countries=['united states'],\n",
        "      categories=['monetary policy', 'inflation', 'employment'],\n",
        "      importances=['high']\n",
        ")\n",
        "\n",
        "calendar_df = calendar_df[~calendar_df['importance'].isna()].reset_index(drop=True)\n",
        "calendar_df.tail()\n"
      ],
      "metadata": {
        "id": "2IBARAw-_XD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "9iuXIf8id7Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "def eda_indices_dict(ticker_data_dict):\n",
        "    \"\"\"\n",
        "    Perform EDA on dictionary of DataFrame indices from yfinance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_data_dict : dict\n",
        "        Dictionary with ticker symbols as keys and their DataFrames as values\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Market Indices ===\\n\")\n",
        "\n",
        "    # Summary statistics for each index\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        print(f\"\\n--- {display_name} ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of trading days: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Calculate returns\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        returns = df[close_col].pct_change()\n",
        "\n",
        "        # Summary statistics for close prices\n",
        "        print(f\"\\nClose price statistics:\")\n",
        "        print(f\"Mean: {df[close_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[close_col].std():.2f}\")\n",
        "        print(f\"Min: {df[close_col].min():.2f}\")\n",
        "        print(f\"Max: {df[close_col].max():.2f}\")\n",
        "\n",
        "        # Return statistics\n",
        "        print(f\"\\nDaily return statistics:\")\n",
        "        print(f\"Mean daily return: {returns.mean():.4%}\")\n",
        "        print(f\"Std dev of returns: {returns.std():.4%}\")\n",
        "        print(f\"Sharpe ratio (annualized): {(returns.mean() / returns.std() * np.sqrt(252)):.2f}\")\n",
        "        print(f\"Skewness: {returns.skew():.2f}\")\n",
        "        print(f\"Kurtosis: {returns.kurtosis():.2f}\")\n",
        "\n",
        "        # Plot closing prices and returns\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "        # Price chart\n",
        "        ax1.plot(df.index, df[close_col])\n",
        "        ax1.set_title(f\"{display_name} - Closing Prices\")\n",
        "        ax1.set_ylabel(\"Price\")\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Returns histogram\n",
        "        ax2.hist(returns.dropna(), bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
        "        ax2.set_title(f\"{display_name} - Return Distribution\")\n",
        "        ax2.set_xlabel(\"Daily Returns\")\n",
        "        ax2.set_ylabel(\"Frequency\")\n",
        "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Correlation analysis between indices\n",
        "    print(\"\\n=== Correlation Analysis ===\")\n",
        "    close_prices_dict = {}\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        close_prices_dict[display_name] = df[close_col]\n",
        "\n",
        "    close_prices_df = pd.DataFrame(close_prices_dict)\n",
        "    correlation_matrix = close_prices_df.pct_change().corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix of Daily Returns\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "eda_indices_dict(ticker_data)\n"
      ],
      "metadata": {
        "id": "3frCFPBBH2ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_fred_data(fred_data_tuple):\n",
        "    \"\"\"\n",
        "    Perform EDA on FRED API data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    fred_data_tuple : tuple\n",
        "        Tuple containing (dataframes_dict, frequencies_dict)\n",
        "    \"\"\"\n",
        "    dataframes_dict, frequencies_dict = fred_data_tuple\n",
        "\n",
        "    print(\"=== EDA for FRED Economic Indicators ===\\n\")\n",
        "\n",
        "    # Summary for each FRED series\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        frequency = frequencies_dict[series_id]\n",
        "        print(f\"\\n--- {series_id} (Frequency: {frequency}) ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of observations: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Summary statistics\n",
        "        value_col = df.columns[0]\n",
        "        print(f\"\\nSummary statistics:\")\n",
        "        print(f\"Mean: {df[value_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[value_col].std():.2f}\")\n",
        "        print(f\"Min: {df[value_col].min():.2f}\")\n",
        "        print(f\"Max: {df[value_col].max():.2f}\")\n",
        "\n",
        "        # Calculate percent change based on frequency\n",
        "        if frequency == 'd':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Daily % Change'\n",
        "        elif frequency == 'w':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Weekly % Change'\n",
        "        elif frequency == 'm':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Monthly % Change'\n",
        "        else:\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = '% Change'\n",
        "\n",
        "        # Remove infinite and NaN values\n",
        "        pct_change_clean = pct_change.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(pct_change_clean) > 0:\n",
        "            print(f\"\\n{change_label} statistics:\")\n",
        "            print(f\"Mean: {pct_change_clean.mean():.4%}\")\n",
        "            print(f\"Std Dev: {pct_change_clean.std():.4%}\")\n",
        "\n",
        "            # Plot time series and change distribution\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "            # Time series plot\n",
        "            ax1.plot(df.index, df[value_col])\n",
        "            ax1.set_title(f\"{series_id} - Time Series\")\n",
        "            ax1.set_ylabel(\"Value\")\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Change distribution\n",
        "            try:\n",
        "                ax2.hist(pct_change_clean, bins=50, alpha=0.75, color='green', edgecolor='black')\n",
        "                ax2.set_title(f\"{series_id} - {change_label} Distribution\")\n",
        "                ax2.set_xlabel(change_label)\n",
        "                ax2.set_ylabel(\"Frequency\")\n",
        "                ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: Could not create histogram for {series_id}: {str(e)}\")\n",
        "                ax2.text(0.5, 0.5, 'Histogram not available\\ndue to data issues',\n",
        "                         ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: No valid {change_label} data available for {series_id}\")\n",
        "\n",
        "    # Combine all FRED data for correlation analysis\n",
        "    print(\"\\n=== Cross-Series Analysis ===\")\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        # Resample all series to monthly frequency for comparison\n",
        "        if frequencies_dict[series_id] == 'd':\n",
        "            resampled = df.resample('M').last()\n",
        "        elif frequencies_dict[series_id] == 'w':\n",
        "            resampled = df.resample('M').last()\n",
        "        else:\n",
        "            resampled = df\n",
        "\n",
        "        combined_df[series_id] = resampled[resampled.columns[0]]\n",
        "\n",
        "    # Calculate correlation matrix with handling for NaN values\n",
        "    combined_pct_change = combined_df.pct_change(fill_method=None)\n",
        "    combined_pct_change_clean = combined_pct_change.replace([np.inf, -np.inf], np.nan)\n",
        "    correlation_matrix = combined_pct_change_clean.corr()\n",
        "\n",
        "    if not correlation_matrix.empty:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "        plt.title(\"Correlation Matrix of Economic Indicators (Monthly % Changes)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Warning: Not enough valid data to create correlation matrix\")\n",
        "\n",
        "eda_fred_data((fred_data[0], fred_data[1]))\n"
      ],
      "metadata": {
        "id": "-TMHwNHFH6_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "def eda_indices_dict(ticker_data_dict):\n",
        "    \"\"\"\n",
        "    Perform EDA on dictionary of DataFrame indices from yfinance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_data_dict : dict\n",
        "        Dictionary with ticker symbols as keys and their DataFrames as values\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Market Indices ===\\n\")\n",
        "\n",
        "    # Find the common start date and individual start dates\n",
        "    all_start_dates = {}\n",
        "    all_end_dates = {}\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        all_start_dates[display_name] = df.index.min()\n",
        "        all_end_dates[display_name] = df.index.max()\n",
        "\n",
        "    common_start_date = max(all_start_dates.values())\n",
        "    common_end_date = min(all_end_dates.values())\n",
        "\n",
        "    print(f\"Common data period (all indices available): {common_start_date.date()} to {common_end_date.date()}\")\n",
        "    print(f\"Total common trading days: {sum(1 for d in pd.date_range(common_start_date, common_end_date, freq='B'))}\")\n",
        "\n",
        "    # Summary statistics for each index\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        print(f\"\\n--- {display_name} ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of trading days: {len(df)}\")\n",
        "\n",
        "        # Check data availability\n",
        "        if df.index.min() > pd.Timestamp('2000-01-01'):\n",
        "            print(f\"⚠️ Data starts after 2000: {df.index.min().date()}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Calculate returns\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        returns = df[close_col].pct_change()\n",
        "\n",
        "        # Summary statistics for close prices\n",
        "        print(f\"\\nClose price statistics:\")\n",
        "        print(f\"Mean: {df[close_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[close_col].std():.2f}\")\n",
        "        print(f\"Min: {df[close_col].min():.2f}\")\n",
        "        print(f\"Max: {df[close_col].max():.2f}\")\n",
        "\n",
        "        # Return statistics\n",
        "        print(f\"\\nDaily return statistics:\")\n",
        "        print(f\"Mean daily return: {returns.mean():.4%}\")\n",
        "        print(f\"Std dev of returns: {returns.std():.4%}\")\n",
        "        print(f\"Sharpe ratio (annualized): {(returns.mean() / returns.std() * np.sqrt(252)):.2f}\")\n",
        "        print(f\"Skewness: {returns.skew():.2f}\")\n",
        "        print(f\"Kurtosis: {returns.kurtosis():.2f}\")\n",
        "\n",
        "        # Plot closing prices and returns\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "        # Price chart\n",
        "        ax1.plot(df.index, df[close_col])\n",
        "        ax1.set_title(f\"{display_name} - Closing Prices\")\n",
        "        ax1.set_ylabel(\"Price\")\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Returns histogram\n",
        "        ax2.hist(returns.dropna(), bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
        "        ax2.set_title(f\"{display_name} - Return Distribution\")\n",
        "        ax2.set_xlabel(\"Daily Returns\")\n",
        "        ax2.set_ylabel(\"Frequency\")\n",
        "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Correlation analysis between indices (using common period)\n",
        "    print(\"\\n=== Correlation Analysis ===\")\n",
        "    close_prices_dict = {}\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        close_prices_dict[display_name] = df[close_col]\n",
        "\n",
        "    close_prices_df = pd.DataFrame(close_prices_dict)\n",
        "\n",
        "    # Common period correlation\n",
        "    common_period_df = close_prices_df.loc[common_start_date:common_end_date]\n",
        "    correlation_matrix_common = common_period_df.pct_change().corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix_common, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title(f\"Correlation Matrix of Daily Returns (Common Period: {common_start_date.date()} to {common_end_date.date()})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # All available data correlation (with missing values)\n",
        "    correlation_matrix_all = close_prices_df.pct_change().corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix_all, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix of Daily Returns (All Available Data)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Data availability timeline\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for i, (display_name, df) in enumerate(ticker_data_dict.items()):\n",
        "        plt.barh(i, (df.index.max() - df.index.min()).days,\n",
        "                left=(df.index.min() - pd.Timestamp('2000-01-01')).days,\n",
        "                height=0.6, label=f\"{display_name}: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "\n",
        "    plt.yticks(range(len(ticker_data_dict)), list(ticker_data_dict.keys()))\n",
        "    plt.xlabel(\"Days since 2000-01-01\")\n",
        "    plt.title(\"Data Availability Timeline for Each Index\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def eda_fred_data(fred_data_tuple):\n",
        "    \"\"\"\n",
        "    Perform EDA on FRED API data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    fred_data_tuple : tuple\n",
        "        Tuple containing (dataframes_dict, frequencies_dict)\n",
        "    \"\"\"\n",
        "    dataframes_dict, frequencies_dict = fred_data_tuple\n",
        "\n",
        "    print(\"=== EDA for FRED Economic Indicators ===\\n\")\n",
        "\n",
        "    # Summary for each FRED series\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        frequency = frequencies_dict[series_id]\n",
        "        print(f\"\\n--- {series_id} (Frequency: {frequency}) ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of observations: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Summary statistics\n",
        "        value_col = df.columns[0]\n",
        "        print(f\"\\nSummary statistics:\")\n",
        "        print(f\"Mean: {df[value_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[value_col].std():.2f}\")\n",
        "        print(f\"Min: {df[value_col].min():.2f}\")\n",
        "        print(f\"Max: {df[value_col].max():.2f}\")\n",
        "\n",
        "        # Calculate percent change based on frequency\n",
        "        if frequency == 'd':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Daily % Change'\n",
        "        elif frequency == 'w':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Weekly % Change'\n",
        "        elif frequency == 'm':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Monthly % Change'\n",
        "        else:\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = '% Change'\n",
        "\n",
        "        # Remove infinite and NaN values\n",
        "        pct_change_clean = pct_change.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(pct_change_clean) > 0:\n",
        "            print(f\"\\n{change_label} statistics:\")\n",
        "            print(f\"Mean: {pct_change_clean.mean():.4%}\")\n",
        "            print(f\"Std Dev: {pct_change_clean.std():.4%}\")\n",
        "\n",
        "            # Plot time series and change distribution\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "            # Time series plot\n",
        "            ax1.plot(df.index, df[value_col])\n",
        "            ax1.set_title(f\"{series_id} - Time Series\")\n",
        "            ax1.set_ylabel(\"Value\")\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Change distribution\n",
        "            try:\n",
        "                ax2.hist(pct_change_clean, bins=50, alpha=0.75, color='green', edgecolor='black')\n",
        "                ax2.set_title(f\"{series_id} - {change_label} Distribution\")\n",
        "                ax2.set_xlabel(change_label)\n",
        "                ax2.set_ylabel(\"Frequency\")\n",
        "                ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: Could not create histogram for {series_id}: {str(e)}\")\n",
        "                ax2.text(0.5, 0.5, 'Histogram not available\\ndue to data issues',\n",
        "                         ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: No valid {change_label} data available for {series_id}\")\n",
        "\n",
        "    # Combine all FRED data for correlation analysis\n",
        "    print(\"\\n=== Cross-Series Analysis ===\")\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        # Resample all series to monthly frequency for comparison\n",
        "        if frequencies_dict[series_id] == 'd':\n",
        "            resampled = df.resample('M').last()\n",
        "        elif frequencies_dict[series_id] == 'w':\n",
        "            resampled = df.resample('M').last()\n",
        "        else:\n",
        "            resampled = df\n",
        "\n",
        "        combined_df[series_id] = resampled[resampled.columns[0]]\n",
        "\n",
        "    # Calculate correlation matrix with handling for NaN values\n",
        "    combined_pct_change = combined_df.pct_change(fill_method=None)\n",
        "    combined_pct_change_clean = combined_pct_change.replace([np.inf, -np.inf], np.nan)\n",
        "    correlation_matrix = combined_pct_change_clean.corr()\n",
        "\n",
        "    if not correlation_matrix.empty:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "        plt.title(\"Correlation Matrix of Economic Indicators (Monthly % Changes)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Warning: Not enough valid data to create correlation matrix\")\n",
        "\n",
        "def eda_calendar_data(calendar_df):\n",
        "    \"\"\"\n",
        "    Perform EDA on economic calendar data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    calendar_df : pandas.DataFrame\n",
        "        DataFrame containing economic calendar data\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Economic Calendar ===\\n\")\n",
        "\n",
        "    # Basic info\n",
        "    print(f\"Date range: {calendar_df['date'].min()} to {calendar_df['date'].max()}\")\n",
        "    print(f\"Total number of events: {len(calendar_df)}\")\n",
        "\n",
        "    # Convert date column to datetime - handle potential type issues\n",
        "    if calendar_df['date'].dtype != 'datetime64[ns]':\n",
        "        try:\n",
        "            # Try converting to string first if necessary\n",
        "            calendar_df['date'] = calendar_df['date'].astype(str)\n",
        "            calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%d/%m/%Y')\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert date column: {e}\")\n",
        "            # Try alternative conversion\n",
        "            try:\n",
        "                calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
        "            except Exception as e2:\n",
        "                print(f\"Error: Unable to convert date column: {e2}\")\n",
        "                return\n",
        "\n",
        "    # Extract year and month for analysis\n",
        "    calendar_df['year'] = calendar_df['date'].dt.year\n",
        "    calendar_df['month'] = calendar_df['date'].dt.month\n",
        "    calendar_df['weekday'] = calendar_df['date'].dt.dayofweek\n",
        "\n",
        "    # Events by type\n",
        "    print(\"\\n--- Event Categories ---\")\n",
        "    event_types = calendar_df['event'].str.extract(r'(.+?)\\s*(?:\\(|\\s*$)')[0].value_counts()\n",
        "    print(event_types.head(15))\n",
        "\n",
        "    # Events by year\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    yearly_events = calendar_df.groupby('year').size()\n",
        "    yearly_events.plot(kind='bar', alpha=0.75, color='blue', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Year\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Events by month\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    monthly_events = calendar_df.groupby('month').size()\n",
        "    monthly_events.plot(kind='bar', alpha=0.75, color='green', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Month\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Events by weekday\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    weekday_events = calendar_df.groupby('weekday').size()\n",
        "    weekday_events.plot(kind='bar', alpha=0.75, color='orange', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Weekday\")\n",
        "    plt.xlabel(\"Weekday\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Time of day analysis\n",
        "    try:\n",
        "        calendar_df['hour'] = pd.to_datetime(calendar_df['time'].astype(str), format='%H:%M').dt.hour\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        hourly_events = calendar_df.groupby('hour').size()\n",
        "        hourly_events.plot(kind='bar', alpha=0.75, color='purple', edgecolor='black')\n",
        "        plt.title(\"Number of Economic Events by Hour of Day\")\n",
        "        plt.xlabel(\"Hour\")\n",
        "        plt.ylabel(\"Number of Events\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not analyze time of day: {e}\")\n",
        "\n",
        "    # # Event importance\n",
        "    # print(\"\\n--- Event Importance ---\")\n",
        "    # importance_counts = calendar_df['importance'].value_counts()\n",
        "    # print(importance_counts)\n",
        "\n",
        "    # Create a heatmap of events by month and year\n",
        "    pivot_table = calendar_df.pivot_table(\n",
        "        values='id',\n",
        "        index='year',\n",
        "        columns='month',\n",
        "        aggfunc='count',\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(pivot_table, cmap='YlOrRd', annot=True, fmt='d')\n",
        "    plt.title(\"Event Count Heatmap by Year and Month\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Year\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Most common event types over time\n",
        "    calendar_df['event_type'] = calendar_df['event'].str.extract(r'(.+?)\\s*(?:\\(|\\s*$)')[0]\n",
        "    top_5_events = event_types.head(5).index\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for event in top_5_events:\n",
        "        event_data = calendar_df[calendar_df['event_type'] == event]\n",
        "        event_by_year = event_data.groupby('year').size()\n",
        "        plt.plot(event_by_year.index, event_by_year.values, marker='o', label=event)\n",
        "\n",
        "    plt.title(\"Top 5 Economic Event Types by Year\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "eda_calendar_data(calendar_df)"
      ],
      "metadata": {
        "id": "0PBXNe6aNQi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "KlRIHTfOeEWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's access the SPY data from your ticker_data dictionary\n",
        "spy_df = ticker_data['SPY (US)'].copy()\n",
        "\n",
        "# Filter to start from the first full market week of 2007\n",
        "# The first trading day of 2007 was January 3rd (Wednesday)\n",
        "# So the first full market week started on January 8th (Monday)\n",
        "start_date = '2007-01-08'\n",
        "spy_df_filtered = spy_df[spy_df.index >= start_date]\n",
        "\n",
        "# Calculate the percent change from open to close\n",
        "spy_df_filtered['pct_change_open_close'] = (spy_df_filtered['SPY_Close'] - spy_df_filtered['SPY_Open']) / spy_df_filtered['SPY_Open'] * 100\n",
        "\n",
        "# Create the target variable trend_i\n",
        "# trend_i = 1 if absolute percent change > 0.5%, else 0\n",
        "spy_df_filtered['trend_i'] = np.where(np.abs(spy_df_filtered['pct_change_open_close']) > 0.5, 1, 0)\n",
        "\n",
        "# Display some summary statistics\n",
        "print(f\"Date range: {spy_df_filtered.index.min().date()} to {spy_df_filtered.index.max().date()}\")\n",
        "print(f\"Total trading days: {len(spy_df_filtered)}\")\n",
        "print(f\"Days with trend (trend_i = 1): {spy_df_filtered['trend_i'].sum()}\")\n",
        "print(f\"Days without trend (trend_i = 0): {len(spy_df_filtered) - spy_df_filtered['trend_i'].sum()}\")\n",
        "print(f\"Percentage of trending days: {spy_df_filtered['trend_i'].mean() * 100:.2f}%\")\n",
        "\n",
        "# Let's create a visualization to understand the distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the distribution of daily percentage changes\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(spy_df_filtered['pct_change_open_close'], bins=100, alpha=0.75, edgecolor='black')\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='0.5% threshold')\n",
        "plt.axvline(x=-0.5, color='red', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Daily % Change (Open to Close)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of SPY Daily Percentage Changes (2007-Present)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a small sample output to verify the calculation\n",
        "print(\"\\nSample of the data with trend_i:\")\n",
        "sample_data = spy_df_filtered[['SPY_Open', 'SPY_Close', 'pct_change_open_close', 'trend_i']].copy()\n",
        "sample_data['abs_pct_change'] = np.abs(sample_data['pct_change_open_close'])\n",
        "print(sample_data.head(10))\n",
        "\n",
        "# Let's also check for balance between trending up and trending down\n",
        "trending_days = spy_df_filtered[spy_df_filtered['trend_i'] == 1]\n",
        "trend_up = trending_days[trending_days['pct_change_open_close'] > 0]\n",
        "trend_down = trending_days[trending_days['pct_change_open_close'] < 0]\n",
        "\n",
        "print(f\"\\nTrending days (|change| > 0.5%): {len(trending_days)}\")\n",
        "print(f\"  Upward trends (change > 0.5%): {len(trend_up)}\")\n",
        "print(f\"  Downward trends (change < -0.5%): {len(trend_down)}\")\n",
        "\n",
        "# Let's also check for any outliers or extreme values\n",
        "print(f\"\\nMaximum daily % change: {spy_df_filtered['pct_change_open_close'].max():.2f}%\")\n",
        "print(f\"Minimum daily % change: {spy_df_filtered['pct_change_open_close'].min():.2f}%\")\n",
        "\n",
        "# Create your base DataFrame for feature engineering\n",
        "base_df = spy_df_filtered.copy()\n",
        "print(f\"\\nBase DataFrame shape: {base_df.shape}\")\n",
        "print(f\"Columns: {base_df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "mQfr6ozmcoPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's create a list of indices to include (excluding VIX Brazil)\n",
        "indices_to_include = [key for key in ticker_data.keys() if key != 'VIX Brazil']\n",
        "\n",
        "# Let's create a function to extract and rename the relevant columns\n",
        "def extract_columns(df, display_name):\n",
        "    # Extract ticker symbol from the column names\n",
        "    ticker_symbol = df.columns[0].split('_')[0]\n",
        "\n",
        "    # Extract relevant columns and rename them\n",
        "    columns_to_extract = {}\n",
        "\n",
        "    if f'{ticker_symbol}_Open' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Open'] = f'{display_name}_Open'\n",
        "    if f'{ticker_symbol}_Close' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Close'] = f'{display_name}_Close'\n",
        "    if f'{ticker_symbol}_Volume' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Volume'] = f'{display_name}_Volume'\n",
        "\n",
        "    # Create a new dataframe with only the relevant columns\n",
        "    extracted_df = df[list(columns_to_extract.keys())].copy()\n",
        "    extracted_df = extracted_df.rename(columns=columns_to_extract)\n",
        "\n",
        "    return extracted_df\n",
        "\n",
        "# Join the data from other indices to the base dataframe\n",
        "for display_name in indices_to_include:\n",
        "    if display_name != 'SPY (US)':  # We already have SPY in the base_df\n",
        "        index_df = ticker_data[display_name]\n",
        "\n",
        "        # Filter to match the date range of base_df\n",
        "        index_df_filtered = index_df[index_df.index >= start_date]\n",
        "\n",
        "        # Extract the relevant columns\n",
        "        extracted_df = extract_columns(index_df_filtered, display_name)\n",
        "\n",
        "        # Join to base_df\n",
        "        base_df = base_df.join(extracted_df, how='left')\n",
        "\n",
        "# Display the resulting dataframe structure\n",
        "print(f\"Base DataFrame shape after joining indices: {base_df.shape}\")\n",
        "print(f\"\\nColumns in base_df:\")\n",
        "for col in base_df.columns:\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "# Check for missing values in the joined data\n",
        "missing_summary = base_df.isnull().sum()\n",
        "if missing_summary.any():\n",
        "    print(\"\\nMissing values in joined data:\")\n",
        "    print(missing_summary[missing_summary > 0])\n",
        "\n",
        "# Sample of the data to verify the join\n",
        "print(\"\\nSample of the joined data:\")\n",
        "sample_columns = ['SPY_Open', 'SPY_Close', 'pct_change_open_close', 'trend_i']\n",
        "# Add some other index columns to the sample\n",
        "for display_name in indices_to_include[:3]:  # Show first 3 indices as example\n",
        "    if display_name != 'SPY (US)':\n",
        "        open_col = f'{display_name}_Open'\n",
        "        if open_col in base_df.columns:\n",
        "            sample_columns.append(open_col)\n",
        "\n",
        "print(base_df[sample_columns].head())\n",
        "\n",
        "# Summary of data availability for each index\n",
        "print(\"\\nData availability summary:\")\n",
        "for display_name in indices_to_include:\n",
        "    if display_name != 'SPY (US)':\n",
        "        open_col = f'{display_name}_Open'\n",
        "        if open_col in base_df.columns:\n",
        "            non_null_count = base_df[open_col].count()\n",
        "            total_rows = len(base_df)\n",
        "            coverage = (non_null_count / total_rows) * 100\n",
        "            print(f\"{display_name}: {non_null_count}/{total_rows} ({coverage:.1f}% coverage)\")"
      ],
      "metadata": {
        "id": "UyJ9eUyKemft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the original base_df\n",
        "base_df_10am = base_df.copy()\n",
        "\n",
        "# Define market groups\n",
        "asian_markets = ['Nikkei 225 (Japan)', 'Hang Seng (Hong Kong)', 'SSE Composite (China)', 'ASX 200 (Australia)']\n",
        "european_markets = ['DAX (Germany)', 'FTSE 100 (UK)', 'CAC 40 (France)', 'Euro Stoxx 50 (EU)']\n",
        "us_markets = ['SPY']\n",
        "currency_pairs = ['EUR/USD', 'JPY/USD', 'CNY/USD']\n",
        "commodities = ['Gold', 'Crude Oil', 'Silver', 'Corn', 'Copper']\n",
        "volatility_indices = ['VIX (US)',  'US Dollar Index']\n",
        "\n",
        "# First, create lagged versions of ALL columns\n",
        "for col in base_df.columns:\n",
        "    base_df_10am[f'{col}_lag1'] = base_df[col].shift(1)\n",
        "\n",
        "# Now, create current day columns for specific markets\n",
        "# Asian markets - current day Open, Close, Volume\n",
        "for market in asian_markets:\n",
        "    for col_type in ['Open', 'Close', 'Volume']:\n",
        "        col_name = f'{market}_{col_type}'\n",
        "        if col_name in base_df.columns:\n",
        "            base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# European markets - current day Open only\n",
        "for market in european_markets:\n",
        "    col_name = f'{market}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# SPY - current day Open\n",
        "if 'SPY_Open' in base_df.columns:\n",
        "    base_df_10am['SPY_Open_current'] = base_df['SPY_Open']\n",
        "\n",
        "# Volatility indices - current day Open\n",
        "for market in volatility_indices:\n",
        "    col_name = f'{market}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# Currency pairs - current day Open\n",
        "for pair in currency_pairs:\n",
        "    col_name = f'{pair}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# Commodities - current day Open\n",
        "for commodity in commodities:\n",
        "    col_name = f'{commodity}_Open'\n",
        "    if col_name in base_df.columns:\n",
        "        base_df_10am[f'{col_name}_current'] = base_df[col_name]\n",
        "\n",
        "# Target variable - current day\n",
        "if 'trend_i' in base_df.columns:\n",
        "    base_df_10am['trend_i_current'] = base_df['trend_i']\n",
        "\n",
        "# Now let's select only the columns we want to keep\n",
        "columns_to_keep = []\n",
        "\n",
        "# Keep lagged versions of everything\n",
        "for col in base_df.columns:\n",
        "    columns_to_keep.append(f'{col}_lag1')\n",
        "\n",
        "# Keep current day Asian Open, Close, Volume\n",
        "for market in asian_markets:\n",
        "    for col_type in ['Open', 'Close', 'Volume']:\n",
        "        col_name = f'{market}_{col_type}_current'\n",
        "        if col_name in base_df_10am.columns:\n",
        "            columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day European Open\n",
        "for market in european_markets:\n",
        "    col_name = f'{market}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day SPY Open\n",
        "if 'SPY_Open_current' in base_df_10am.columns:\n",
        "    columns_to_keep.append('SPY_Open_current')\n",
        "\n",
        "# Keep current day Volatility indices Open\n",
        "for market in volatility_indices:\n",
        "    col_name = f'{market}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day Currency pairs Open\n",
        "for pair in currency_pairs:\n",
        "    col_name = f'{pair}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day Commodities Open\n",
        "for commodity in commodities:\n",
        "    col_name = f'{commodity}_Open_current'\n",
        "    if col_name in base_df_10am.columns:\n",
        "        columns_to_keep.append(col_name)\n",
        "\n",
        "# Keep current day target variable\n",
        "if 'trend_i_current' in base_df_10am.columns:\n",
        "    columns_to_keep.append('trend_i_current')\n",
        "\n",
        "# Filter to keep only the columns we want\n",
        "base_df_10am = base_df_10am[columns_to_keep]\n",
        "\n",
        "# Drop the first row since it will have NaN values from lagging\n",
        "base_df_10am = base_df_10am.iloc[1:].copy()\n",
        "\n",
        "# Display the structure\n",
        "print(f\"Final dataframe shape: {base_df_10am.shape}\")\n",
        "print(\"\\nColumns in final dataframe:\")\n",
        "for i, col in enumerate(base_df_10am.columns):\n",
        "    print(f\"{i+1:3d}. {col}\")\n",
        "\n",
        "# Verify our structure\n",
        "print(\"\\n=== Data Structure at 10 AM EST ===\")\n",
        "print(\"\\nLagged columns (previous day):\")\n",
        "lagged_cols = [col for col in base_df_10am.columns if '_lag1' in col]\n",
        "print(f\"  Count: {len(lagged_cols)}\")\n",
        "\n",
        "print(\"\\nCurrent day columns:\")\n",
        "current_cols = [col for col in base_df_10am.columns if '_current' in col]\n",
        "print(f\"  Count: {len(current_cols)}\")\n",
        "print(f\"  Open prices: {len([col for col in current_cols if 'Open' in col])}\")\n",
        "print(f\"  Close prices: {len([col for col in current_cols if 'Close' in col])}\")\n",
        "print(f\"  Volume: {len([col for col in current_cols if 'Volume' in col])}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_summary = base_df_10am.isnull().sum()\n",
        "if missing_summary.any():\n",
        "    print(\"\\nMissing values in final dataframe:\")\n",
        "    missing_df = pd.DataFrame({'Missing_Count': missing_summary[missing_summary > 0]})\n",
        "    missing_df['Percentage'] = (missing_df['Missing_Count'] / len(base_df_10am) * 100).round(2)\n",
        "    print(missing_df)"
      ],
      "metadata": {
        "id": "7iCXdfrS3k9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development"
      ],
      "metadata": {
        "id": "6l5IjaqLeHHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Filter data to keep only 2007-2024 (drop 2025)\n",
        "# Filter out 2025 data\n",
        "base_df_filtered = base_df_10am[base_df_10am.index.year < 2025].copy()\n",
        "\n",
        "# Split into train (2007-2023) and test (2024)\n",
        "train_data = base_df_filtered[base_df_filtered.index.year < 2024]\n",
        "test_data = base_df_filtered[base_df_filtered.index.year == 2024]\n",
        "\n",
        "print(f\"Train data period: {train_data.index.min().date()} to {train_data.index.max().date()}\")\n",
        "print(f\"Test data period: {test_data.index.min().date()} to {test_data.index.max().date()}\")\n",
        "print(f\"Train shape: {train_data.shape}, Test shape: {test_data.shape}\")\n"
      ],
      "metadata": {
        "id": "yxW30g14EBDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Prepare the data for modeling by handling missing values and creating X, y\n",
        "    \"\"\"\n",
        "    # Separate features and target for train\n",
        "    X_train = train_df.drop('trend_i_current', axis=1)\n",
        "    y_train = train_df['trend_i_current']\n",
        "\n",
        "    # Separate features and target for test\n",
        "    X_test = test_df.drop('trend_i_current', axis=1)\n",
        "    y_test = test_df['trend_i_current']\n",
        "\n",
        "    # Handle missing values using imputer fitted on training data\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train),\n",
        "                                   columns=X_train.columns,\n",
        "                                   index=X_train.index)\n",
        "    X_test_imputed = pd.DataFrame(imputer.transform(X_test),\n",
        "                                  columns=X_test.columns,\n",
        "                                  index=X_test.index)\n",
        "\n",
        "    # Scale features using scaler fitted on training data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed),\n",
        "                                  columns=X_train.columns,\n",
        "                                  index=X_train.index)\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed),\n",
        "                                 columns=X_test.columns,\n",
        "                                 index=X_test.index)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, imputer"
      ],
      "metadata": {
        "id": "pre1FqVdCncf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, scaler, imputer = prepare_data(train_data, test_data)\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"y_test distribution:\\n{y_test.value_counts()}\")"
      ],
      "metadata": {
        "id": "mmhqhoyHCnfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model performance and visualize results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{model_name} Performance on 2024 Test Data:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "    # Calculate naive predictor accuracy\n",
        "    naive_accuracy = max(y_true.mean(), 1 - y_true.mean())\n",
        "    improvement = (accuracy_score(y_true, y_pred) - naive_accuracy) * 100\n",
        "    print(f\"Naive Predictor Accuracy: {naive_accuracy:.4f}\")\n",
        "    print(f\"Improvement over Naive: {improvement:.2f}%\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{model_name} Confusion Matrix - 2024 Test Data')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    # Return metrics for comparison\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'improvement': improvement,\n",
        "        'confusion_matrix': cm,\n",
        "        'classification_report': classification_report(y_true, y_pred, output_dict=True)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "6YSkDTTCCnh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_logistic_regression(X_train, y_train, X_test, n_splits=5):\n",
        "    \"\"\"\n",
        "    Train Logistic Regression using time series cross-validation for validation,\n",
        "    then train final model on all training data\n",
        "    \"\"\"\n",
        "    # Use TimeSeriesSplit on training data for validation\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    cv_scores = []\n",
        "\n",
        "    # Validate model using time series CV\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
        "        X_train_fold = X_train.iloc[train_idx]\n",
        "        X_val_fold = X_train.iloc[val_idx]\n",
        "        y_train_fold = y_train.iloc[train_idx]\n",
        "        y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "        # Create and train model for this fold\n",
        "        model = LogisticRegression(\n",
        "            random_state=42,\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Validate on fold\n",
        "        y_pred_val = model.predict(X_val_fold)\n",
        "        fold_score = accuracy_score(y_val_fold, y_pred_val)\n",
        "        cv_scores.append(fold_score)\n",
        "\n",
        "        print(f\"Fold {fold + 1}/{n_splits} validation accuracy: {fold_score:.4f}\")\n",
        "\n",
        "    print(f\"Average CV accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Train final model on all training data\n",
        "    final_model = LogisticRegression(\n",
        "        random_state=42,\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = final_model.predict(X_test)\n",
        "\n",
        "    return y_pred, final_model, cv_scores\n",
        "\n",
        "# Train and evaluate Logistic Regression\n",
        "y_pred_lr, model_lr, lr_cv_scores = train_logistic_regression(X_train, y_train, X_test)\n",
        "lr_results = evaluate_model(y_test, y_pred_lr, 'Logistic Regression')\n"
      ],
      "metadata": {
        "id": "mOqv8MyNCnkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgboost(X_train, y_train, X_test, n_splits=5):\n",
        "    \"\"\"\n",
        "    Train XGBoost using time series cross-validation for validation,\n",
        "    then train final model on all training data\n",
        "    \"\"\"\n",
        "    # Use TimeSeriesSplit on training data for validation\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    cv_scores = []\n",
        "\n",
        "    # Validate model using time series CV\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
        "        X_train_fold = X_train.iloc[train_idx]\n",
        "        X_val_fold = X_train.iloc[val_idx]\n",
        "        y_train_fold = y_train.iloc[train_idx]\n",
        "        y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "        # Create and train model for this fold\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Validate on fold\n",
        "        y_pred_val = model.predict(X_val_fold)\n",
        "        fold_score = accuracy_score(y_val_fold, y_pred_val)\n",
        "        cv_scores.append(fold_score)\n",
        "\n",
        "        print(f\"Fold {fold + 1}/{n_splits} validation accuracy: {fold_score:.4f}\")\n",
        "\n",
        "    print(f\"Average CV accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Train final model on all training data\n",
        "    final_model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = final_model.predict(X_test)\n",
        "\n",
        "    return y_pred, final_model, cv_scores\n",
        "\n",
        "# Train and evaluate XGBoost\n",
        "y_pred_xgb, model_xgb, xgb_cv_scores = train_xgboost(X_train, y_train, X_test)\n",
        "xgb_results = evaluate_model(y_test, y_pred_xgb, 'XGBoost')"
      ],
      "metadata": {
        "id": "ClUv3UEFDZMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LSTM specific layers\n",
        "from tensorflow.keras.layers import LSTM, Dropout\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Define LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    \"\"\"\n",
        "    Create an LSTM model\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Train LSTM function\n",
        "def train_lstm(X_train, y_train, X_test, n_splits=5, epochs=30, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train LSTM using time series cross-validation for validation,\n",
        "    then train final model on all training data\n",
        "    \"\"\"\n",
        "    # Use TimeSeriesSplit on training data for validation\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    cv_scores = []\n",
        "\n",
        "    # Reshape data for LSTM (samples, timesteps, features)\n",
        "    X_train_lstm = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "    X_test_lstm = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced',\n",
        "                                       classes=np.unique(y_train),\n",
        "                                       y=y_train)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # Validate model using time series CV\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
        "        X_train_fold = X_train_lstm[train_idx]\n",
        "        X_val_fold = X_train_lstm[val_idx]\n",
        "        y_train_fold = y_train.iloc[train_idx]\n",
        "        y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "        # Create and compile model\n",
        "        model = create_lstm_model(input_shape=(1, X_train.shape[1]))\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train_fold, y_train_fold,\n",
        "            validation_data=(X_val_fold, y_val_fold),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=[early_stopping],\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Validate on fold\n",
        "        y_pred_proba = model.predict(X_val_fold, verbose=0)\n",
        "        y_pred_val = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "        fold_score = accuracy_score(y_val_fold, y_pred_val)\n",
        "        cv_scores.append(fold_score)\n",
        "\n",
        "        print(f\"Fold {fold + 1}/{n_splits} validation accuracy: {fold_score:.4f}\")\n",
        "\n",
        "        # Clear session to prevent memory issues\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    print(f\"Average CV accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Train final model on all training data\n",
        "    final_model = create_lstm_model(input_shape=(1, X_train.shape[1]))\n",
        "    final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks for final model\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Train final model\n",
        "    history = final_model.fit(\n",
        "        X_train_lstm, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred_proba = final_model.predict(X_test_lstm, verbose=0)\n",
        "\n",
        "    # Find optimal threshold\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "    print(f\"Optimal threshold: {optimal_threshold:.3f}\")\n",
        "\n",
        "    # Use optimal threshold for final predictions\n",
        "    y_pred = (y_pred_proba > optimal_threshold).astype(int).flatten()\n",
        "\n",
        "    return y_pred, final_model, cv_scores\n",
        "\n",
        "# Train and evaluate LSTM\n",
        "print(\"Training LSTM model...\")\n",
        "y_pred_lstm, model_lstm, lstm_cv_scores = train_lstm(X_train, y_train, X_test)\n",
        "lstm_results = evaluate_model(y_test, y_pred_lstm, 'LSTM')"
      ],
      "metadata": {
        "id": "-hgxx6OTDfXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "4ug4MBM5eN85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Compare all models\n",
        "def compare_models(results_dict):\n",
        "    \"\"\"\n",
        "    Compare the performance of all models\n",
        "    \"\"\"\n",
        "    model_names = list(results_dict.keys())\n",
        "    accuracies = [results_dict[model]['accuracy'] for model in model_names]\n",
        "    improvements = [results_dict[model]['improvement'] for model in model_names]\n",
        "\n",
        "    # Create comparison chart\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Accuracy comparison\n",
        "    bars1 = ax1.bar(model_names, accuracies)\n",
        "    ax1.set_title('Model Comparison - Accuracy on 2024 Test Data')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, accuracy in zip(bars1, accuracies):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{accuracy:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # Improvement comparison\n",
        "    bars2 = ax2.bar(model_names, improvements, color=['green' if imp > 0 else 'red' for imp in improvements])\n",
        "    ax2.set_title('Model Comparison - Improvement over Naive Predictor')\n",
        "    ax2.set_ylabel('Improvement (%)')\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax2.axhline(y=12, color='blue', linestyle='--', linewidth=2, label='Full Credit Threshold (12%)')\n",
        "    ax2.legend()\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, improvement in zip(bars2, improvements):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5 if improvement > 0 else bar.get_height() - 1,\n",
        "                f'{improvement:.1f}%', ha='center', va='bottom' if improvement > 0 else 'top')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Compare all models\n",
        "results = {\n",
        "    'Logistic Regression': lr_results,\n",
        "    'XGBoost': xgb_results,\n",
        "    'lstm': lstm_results\n",
        "}\n",
        "\n",
        "compare_models(results)\n",
        "\n",
        "# Cell 12: Print summary of results\n",
        "print(\"=\" * 50)\n",
        "print(\"MODEL PERFORMANCE SUMMARY ON 2024 TEST DATA\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, result in results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"  Improvement over Naive: {result['improvement']:.2f}%\")\n",
        "    print(f\"  Precision (class 1): {result['classification_report']['1']['precision']:.4f}\")\n",
        "    print(f\"  Recall (class 1): {result['classification_report']['1']['recall']:.4f}\")\n",
        "    print(f\"  F1-score (class 1): {result['classification_report']['1']['f1-score']:.4f}\")\n",
        "\n",
        "    # Check if meets assignment requirements\n",
        "    if result['improvement'] >= 12:\n",
        "        print(f\"  STATUS: MEETS FULL CREDIT REQUIREMENT (≥12% improvement)\")\n",
        "    else:\n",
        "        print(f\"  STATUS: Needs {12 - result['improvement']:.2f}% more improvement for full credit\")"
      ],
      "metadata": {
        "id": "dExarGOxd6QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGMm6ZXKJRo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}