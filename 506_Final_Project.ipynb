{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/If80xUyWHGExfWJZQO7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobmillerforever/ECON_506/blob/main/506_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction & Setup"
      ],
      "metadata": {
        "id": "kegfoQH7dxYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fredapi\n",
        "!pip install investpy\n",
        "!pip install ta\n",
        "!pip install keras_tuner"
      ],
      "metadata": {
        "id": "zimnwBagdHqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(f\"GPUs available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")"
      ],
      "metadata": {
        "id": "NidJ2g_MCRlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import datetime as dt\n",
        "from fredapi import Fred\n",
        "import investpy\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, MaxPooling1D, GlobalAveragePooling1D, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import LSTM, Dropout\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ta"
      ],
      "metadata": {
        "id": "7IStQ_96dMOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection & Preparation"
      ],
      "metadata": {
        "id": "lenAt87od0vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ticker Data from yfinance"
      ],
      "metadata": {
        "id": "JPv518tk38lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ticker_data(ticker_dict, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetches data for multiple tickers and creates a DataFrame for each with\n",
        "    single-index columns named as Ticker_ColumnName (e.g., SPY_Close)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_dict : dict\n",
        "        Dictionary with display names as keys and ticker symbols as values\n",
        "    start_date : str\n",
        "        Start date in format 'YYYY-MM-DD'\n",
        "    end_date : str\n",
        "        End date in format 'YYYY-MM-DD'\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with display names as keys and their respective DataFrames as values\n",
        "    \"\"\"\n",
        "    ticker_dataframes = {}\n",
        "\n",
        "    for display_name, ticker_symbol in ticker_dict.items():\n",
        "        # Fetch data for current ticker\n",
        "        data = yf.download(ticker_symbol, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "        # Handle multi-index columns if present\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            # Flatten the multi-index columns to single index\n",
        "            data.columns = [f\"{ticker_symbol}_{col[0]}\" for col in data.columns]\n",
        "        else:\n",
        "            # If not multi-index, still rename columns to match pattern\n",
        "            data.columns = [f\"{ticker_symbol}_{col}\" for col in data.columns]\n",
        "\n",
        "        # Store the DataFrame in the dictionary with display name as key\n",
        "        ticker_dataframes[display_name] = data\n",
        "\n",
        "    return ticker_dataframes\n",
        "\n",
        "tickers = {\n",
        "    # Global Indices\n",
        "    'Nikkei 225 (Japan)': '^N225',\n",
        "    'Hang Seng (Hong Kong)': '^HSI',\n",
        "    'SSE Composite (China)': '000001.SS',\n",
        "    'ASX 200 (Australia)': '^AXJO',\n",
        "    'DAX (Germany)': '^GDAXI',\n",
        "    'FTSE 100 (UK)': '^FTSE',\n",
        "    'CAC 40 (France)': '^FCHI',\n",
        "    'Euro Stoxx 50 (EU)': '^STOXX50E',\n",
        "    'SPY (US)': 'SPY',\n",
        "\n",
        "\n",
        "    # Volatility Indices\n",
        "    'VIX (US)': '^VIX',\n",
        "    #'VIX Brazil': '^VXEWZ',\n",
        "    #'DAX Volatility': '^VDAX',\n",
        "\n",
        "    # Currency Pairs\n",
        "    'US Dollar Index': 'DX-Y.NYB',\n",
        "    #'EUR/USD': 'EURUSD=X',\n",
        "    #'JPY/USD': 'JPY=X',\n",
        "    #'CNY/USD': 'CNY=X',\n",
        "\n",
        "    # Commodities\n",
        "    #'Gold': 'GC=F',\n",
        "    #'Crude Oil': 'CL=F',\n",
        "    #'Silver': 'SI=F',\n",
        "    #'Corn': 'ZC=F',\n",
        "    #'Copper': 'HG=F'\n",
        "}\n",
        "\n",
        "start_date = '2000-01-01'\n",
        "end_date = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "# Get individual DataFrames for each ticker\n",
        "ticker_data = get_ticker_data(tickers, start_date, end_date)\n",
        "\n",
        "# Display the first few rows and column names for each DataFrame\n",
        "for display_name, df in ticker_data.items():\n",
        "    print(f\"\\n{display_name} DataFrame:\")\n",
        "    print(f\"Column names: {df.columns.tolist()}\")\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "gl-IDnFda5K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Economic Indicators from FRED API"
      ],
      "metadata": {
        "id": "3oye3ZLa4CCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fred_data(api_key, series_list, start_date='2000-01-01', end_date=None):\n",
        "    \"\"\"\n",
        "    Fetches data for multiple FRED series at the highest available frequency\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    api_key : str\n",
        "        Your FRED API key\n",
        "    series_list : list\n",
        "        List of FRED series IDs as strings\n",
        "    start_date : str, optional\n",
        "        Start date in format 'YYYY-MM-DD', defaults to '2000-01-01'\n",
        "    end_date : str, optional\n",
        "        End date in format 'YYYY-MM-DD', defaults to current date\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary with series IDs as keys and their respective DataFrames as values\n",
        "    dict\n",
        "        Dictionary with series IDs as keys and the frequency used as values\n",
        "    \"\"\"\n",
        "    # Initialize FRED API connection\n",
        "    fred = Fred(api_key=api_key)\n",
        "\n",
        "    # Set end date to current date if not provided\n",
        "    if end_date is None:\n",
        "        end_date = dt.datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Convert start and end dates to datetime objects\n",
        "    start_dt = dt.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end_dt = dt.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "\n",
        "    # Initialize dictionaries to store DataFrames and frequencies\n",
        "    fred_dataframes = {}\n",
        "    fred_frequencies = {}\n",
        "\n",
        "    # Frequency hierarchy from highest to lowest resolution\n",
        "    # Not all series support all frequencies\n",
        "    frequency_hierarchy = ['d', 'w', 'bw', 'm', 'q', 'sa', 'a']\n",
        "\n",
        "    # Process each series ID\n",
        "    for series_id in series_list:\n",
        "        # Try frequencies in order from highest to lowest resolution\n",
        "        for freq in frequency_hierarchy:\n",
        "            try:\n",
        "                # Get data for current series with current frequency\n",
        "                data = fred.get_series(series_id, start_dt, end_dt, frequency=freq)\n",
        "\n",
        "                # If successful and data is not empty, convert to DataFrame\n",
        "                if not data.empty:\n",
        "                    # Convert Series to DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "                    df.columns = [f\"{series_id}_value\"]\n",
        "\n",
        "                    # Add to dictionaries\n",
        "                    fred_dataframes[series_id] = df\n",
        "                    fred_frequencies[series_id] = freq\n",
        "\n",
        "                    print(f\"Successfully fetched data for {series_id} with frequency '{freq}'\")\n",
        "                    # Break out of frequency loop once we've found a working frequency\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"No data found for {series_id} with frequency '{freq}'\")\n",
        "            except Exception as e:\n",
        "                # If this frequency doesn't work, try the next one\n",
        "                print(f\"Could not fetch {series_id} with frequency '{freq}': {str(e)}\")\n",
        "\n",
        "        # Check if we were able to fetch this series with any frequency\n",
        "        if series_id not in fred_dataframes:\n",
        "            print(f\"Failed to fetch data for {series_id} with any available frequency\")\n",
        "\n",
        "    return fred_dataframes, fred_frequencies\n",
        "\n",
        "from google.colab import userdata\n",
        "fred_api = '8b000b950d5841b5b7e35ebbcacedaea'\n",
        "\n",
        "fred_series = [\n",
        "    'DFF',           # Federal Funds Rate\n",
        "    'T10Y2Y',        # 10-Year minus 2-Year Treasury Spread\n",
        "    'CPIAUCSL',      # Consumer Price Index\n",
        "    'UNRATE',        # Unemployment Rate\n",
        "    'STLFSI',        # St. Louis Fed Financial Stress Index\n",
        "    'M2SL',          # M2 Money Supply\n",
        "    'USSLIND',       # US Leading Index\n",
        "    'BAMLH0A0HYM2',  # High Yield Spread\n",
        "    'GS5',           # 5-Year Treasury Rate\n",
        "    'GS30',          # 30-Year Treasury Rate\n",
        "    'BAMLC0A0CM'     # Corporate Bond Spread\n",
        "]\n",
        "\n",
        "fred_data = get_fred_data(fred_api, fred_series)"
      ],
      "metadata": {
        "id": "j3dC6dlDcMT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fred_data"
      ],
      "metadata": {
        "id": "wy7UUf7W7jz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calendar Dates from investing.com"
      ],
      "metadata": {
        "id": "TE_SJX7_4Gtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calendar_df = investpy.economic_calendar(\n",
        "      from_date='01/01/2000',\n",
        "      to_date='31/12/2025',\n",
        "      countries=['united states'],\n",
        "      categories=None,\n",
        "      importances=['high']\n",
        ")\n",
        "\n",
        "calendar_df = calendar_df[~calendar_df['importance'].isna()].reset_index(drop=True)\n",
        "calendar_df.tail()\n"
      ],
      "metadata": {
        "id": "2IBARAw-_XD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calendar_df['event'].unique()"
      ],
      "metadata": {
        "id": "C50PFxFYJmIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calendar_df[calendar_df['event'] == 'FOMC Economic Projections']"
      ],
      "metadata": {
        "id": "vuU20qKTZSei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "9iuXIf8id7Bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indices EDA"
      ],
      "metadata": {
        "id": "tRKqg7dO4NFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_indices_dict(ticker_data_dict):\n",
        "    \"\"\"\n",
        "    Perform EDA on dictionary of DataFrame indices from yfinance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    ticker_data_dict : dict\n",
        "        Dictionary with ticker symbols as keys and their DataFrames as values\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Market Indices ===\\n\")\n",
        "\n",
        "    # Summary statistics for each index\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        print(f\"\\n--- {display_name} ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of trading days: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Calculate returns\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        returns = df[close_col].pct_change()\n",
        "\n",
        "        # Summary statistics for close prices\n",
        "        print(f\"\\nClose price statistics:\")\n",
        "        print(f\"Mean: {df[close_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[close_col].std():.2f}\")\n",
        "        print(f\"Min: {df[close_col].min():.2f}\")\n",
        "        print(f\"Max: {df[close_col].max():.2f}\")\n",
        "\n",
        "        # Return statistics\n",
        "        print(f\"\\nDaily return statistics:\")\n",
        "        print(f\"Mean daily return: {returns.mean():.4%}\")\n",
        "        print(f\"Std dev of returns: {returns.std():.4%}\")\n",
        "        print(f\"Sharpe ratio (annualized): {(returns.mean() / returns.std() * np.sqrt(252)):.2f}\")\n",
        "        print(f\"Skewness: {returns.skew():.2f}\")\n",
        "        print(f\"Kurtosis: {returns.kurtosis():.2f}\")\n",
        "\n",
        "        # Plot closing prices and returns\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "        # Price chart\n",
        "        ax1.plot(df.index, df[close_col])\n",
        "        ax1.set_title(f\"{display_name} - Closing Prices\")\n",
        "        ax1.set_ylabel(\"Price\")\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Returns histogram\n",
        "        ax2.hist(returns.dropna(), bins=100, alpha=0.75, color='blue', edgecolor='black')\n",
        "        ax2.set_title(f\"{display_name} - Return Distribution\")\n",
        "        ax2.set_xlabel(\"Daily Returns\")\n",
        "        ax2.set_ylabel(\"Frequency\")\n",
        "        ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Correlation analysis between indices\n",
        "    print(\"\\n=== Correlation Analysis ===\")\n",
        "    close_prices_dict = {}\n",
        "    for display_name, df in ticker_data_dict.items():\n",
        "        close_col = [col for col in df.columns if 'Close' in col][0]\n",
        "        close_prices_dict[display_name] = df[close_col]\n",
        "\n",
        "    close_prices_df = pd.DataFrame(close_prices_dict)\n",
        "    correlation_matrix = close_prices_df.pct_change().corr()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix of Daily Returns\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "eda_indices_dict(ticker_data)\n"
      ],
      "metadata": {
        "id": "3frCFPBBH2ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FRED EDA"
      ],
      "metadata": {
        "id": "m4NSTUro4ZCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_fred_data(fred_data_tuple):\n",
        "    \"\"\"\n",
        "    Perform EDA on FRED API data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    fred_data_tuple : tuple\n",
        "        Tuple containing (dataframes_dict, frequencies_dict)\n",
        "    \"\"\"\n",
        "    dataframes_dict, frequencies_dict = fred_data_tuple\n",
        "\n",
        "    print(\"=== EDA for FRED Economic Indicators ===\\n\")\n",
        "\n",
        "    # Summary for each FRED series\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        frequency = frequencies_dict[series_id]\n",
        "        print(f\"\\n--- {series_id} (Frequency: {frequency}) ---\")\n",
        "        print(f\"Data range: {df.index.min().date()} to {df.index.max().date()}\")\n",
        "        print(f\"Number of observations: {len(df)}\")\n",
        "\n",
        "        # Handle missing data\n",
        "        missing_data = df.isnull().sum()\n",
        "        if missing_data.any():\n",
        "            print(\"\\nMissing values:\")\n",
        "            print(missing_data[missing_data > 0])\n",
        "\n",
        "        # Summary statistics\n",
        "        value_col = df.columns[0]\n",
        "        print(f\"\\nSummary statistics:\")\n",
        "        print(f\"Mean: {df[value_col].mean():.2f}\")\n",
        "        print(f\"Std Dev: {df[value_col].std():.2f}\")\n",
        "        print(f\"Min: {df[value_col].min():.2f}\")\n",
        "        print(f\"Max: {df[value_col].max():.2f}\")\n",
        "\n",
        "        # Calculate percent change based on frequency\n",
        "        if frequency == 'd':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Daily % Change'\n",
        "        elif frequency == 'w':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Weekly % Change'\n",
        "        elif frequency == 'm':\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = 'Monthly % Change'\n",
        "        else:\n",
        "            pct_change = df[value_col].pct_change(fill_method=None)\n",
        "            change_label = '% Change'\n",
        "\n",
        "        # Remove infinite and NaN values\n",
        "        pct_change_clean = pct_change.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "        if len(pct_change_clean) > 0:\n",
        "            print(f\"\\n{change_label} statistics:\")\n",
        "            print(f\"Mean: {pct_change_clean.mean():.4%}\")\n",
        "            print(f\"Std Dev: {pct_change_clean.std():.4%}\")\n",
        "\n",
        "            # Plot time series and change distribution\n",
        "            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "            # Time series plot\n",
        "            ax1.plot(df.index, df[value_col])\n",
        "            ax1.set_title(f\"{series_id} - Time Series\")\n",
        "            ax1.set_ylabel(\"Value\")\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Change distribution\n",
        "            try:\n",
        "                ax2.hist(pct_change_clean, bins=50, alpha=0.75, color='green', edgecolor='black')\n",
        "                ax2.set_title(f\"{series_id} - {change_label} Distribution\")\n",
        "                ax2.set_xlabel(change_label)\n",
        "                ax2.set_ylabel(\"Frequency\")\n",
        "                ax2.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "            except ValueError as e:\n",
        "                print(f\"Warning: Could not create histogram for {series_id}: {str(e)}\")\n",
        "                ax2.text(0.5, 0.5, 'Histogram not available\\ndue to data issues',\n",
        "                         ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: No valid {change_label} data available for {series_id}\")\n",
        "\n",
        "    # Combine all FRED data for correlation analysis\n",
        "    print(\"\\n=== Cross-Series Analysis ===\")\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    for series_id, df in dataframes_dict.items():\n",
        "        # Resample all series to monthly frequency for comparison\n",
        "        if frequencies_dict[series_id] == 'd':\n",
        "            resampled = df.resample('M').last()\n",
        "        elif frequencies_dict[series_id] == 'w':\n",
        "            resampled = df.resample('M').last()\n",
        "        else:\n",
        "            resampled = df\n",
        "\n",
        "        combined_df[series_id] = resampled[resampled.columns[0]]\n",
        "\n",
        "    # Calculate correlation matrix with handling for NaN values\n",
        "    combined_pct_change = combined_df.pct_change(fill_method=None)\n",
        "    combined_pct_change_clean = combined_pct_change.replace([np.inf, -np.inf], np.nan)\n",
        "    correlation_matrix = combined_pct_change_clean.corr()\n",
        "\n",
        "    if not correlation_matrix.empty:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
        "        plt.title(\"Correlation Matrix of Economic Indicators (Monthly % Changes)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Warning: Not enough valid data to create correlation matrix\")\n",
        "\n",
        "eda_fred_data((fred_data[0], fred_data[1]))\n"
      ],
      "metadata": {
        "id": "-TMHwNHFH6_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calendar EDA"
      ],
      "metadata": {
        "id": "uEYJ-Rs74kLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_calendar_data(calendar_df):\n",
        "    \"\"\"\n",
        "    Perform EDA on economic calendar data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    calendar_df : pandas.DataFrame\n",
        "        DataFrame containing economic calendar data\n",
        "    \"\"\"\n",
        "    print(\"=== EDA for Economic Calendar ===\\n\")\n",
        "\n",
        "    # Basic info\n",
        "    print(f\"Date range: {calendar_df['date'].min()} to {calendar_df['date'].max()}\")\n",
        "    print(f\"Total number of events: {len(calendar_df)}\")\n",
        "\n",
        "    # Convert date column to datetime - handle potential type issues\n",
        "    if calendar_df['date'].dtype != 'datetime64[ns]':\n",
        "        try:\n",
        "            # Try converting to string first if necessary\n",
        "            calendar_df['date'] = calendar_df['date'].astype(str)\n",
        "            calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%d/%m/%Y')\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not convert date column: {e}\")\n",
        "            # Try alternative conversion\n",
        "            try:\n",
        "                calendar_df['date'] = pd.to_datetime(calendar_df['date'])\n",
        "            except Exception as e2:\n",
        "                print(f\"Error: Unable to convert date column: {e2}\")\n",
        "                return\n",
        "\n",
        "    # Extract year and month for analysis\n",
        "    calendar_df['year'] = calendar_df['date'].dt.year\n",
        "    calendar_df['month'] = calendar_df['date'].dt.month\n",
        "    calendar_df['weekday'] = calendar_df['date'].dt.dayofweek\n",
        "\n",
        "    # Events by type\n",
        "    print(\"\\n--- Event Categories ---\")\n",
        "    event_types = calendar_df['event'].str.extract(r'(.+?)\\s*(?:\\(|\\s*$)')[0].value_counts()\n",
        "    print(event_types.head(15))\n",
        "\n",
        "    # Events by year\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    yearly_events = calendar_df.groupby('year').size()\n",
        "    yearly_events.plot(kind='bar', alpha=0.75, color='blue', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Year\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Events by month\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    monthly_events = calendar_df.groupby('month').size()\n",
        "    monthly_events.plot(kind='bar', alpha=0.75, color='green', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Month\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.xticks(range(12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Events by weekday\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    weekday_events = calendar_df.groupby('weekday').size()\n",
        "    weekday_events.plot(kind='bar', alpha=0.75, color='orange', edgecolor='black')\n",
        "    plt.title(\"Number of Economic Events by Weekday\")\n",
        "    plt.xlabel(\"Weekday\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Time of day analysis\n",
        "    try:\n",
        "        calendar_df['hour'] = pd.to_datetime(calendar_df['time'].astype(str), format='%H:%M').dt.hour\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        hourly_events = calendar_df.groupby('hour').size()\n",
        "        hourly_events.plot(kind='bar', alpha=0.75, color='purple', edgecolor='black')\n",
        "        plt.title(\"Number of Economic Events by Hour of Day\")\n",
        "        plt.xlabel(\"Hour\")\n",
        "        plt.ylabel(\"Number of Events\")\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not analyze time of day: {e}\")\n",
        "\n",
        "    # # Event importance\n",
        "    # print(\"\\n--- Event Importance ---\")\n",
        "    # importance_counts = calendar_df['importance'].value_counts()\n",
        "    # print(importance_counts)\n",
        "\n",
        "    # Create a heatmap of events by month and year\n",
        "    pivot_table = calendar_df.pivot_table(\n",
        "        values='id',\n",
        "        index='year',\n",
        "        columns='month',\n",
        "        aggfunc='count',\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(pivot_table, cmap='YlOrRd', annot=True, fmt='d')\n",
        "    plt.title(\"Event Count Heatmap by Year and Month\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Year\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Most common event types over time\n",
        "    calendar_df['event_type'] = calendar_df['event'].str.extract(r'(.+?)\\s*(?:\\(|\\s*$)')[0]\n",
        "    top_5_events = event_types.head(5).index\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for event in top_5_events:\n",
        "        event_data = calendar_df[calendar_df['event_type'] == event]\n",
        "        event_by_year = event_data.groupby('year').size()\n",
        "        plt.plot(event_by_year.index, event_by_year.values, marker='o', label=event)\n",
        "\n",
        "    plt.title(\"Top 5 Economic Event Types by Year\")\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "eda_calendar_data(calendar_df)"
      ],
      "metadata": {
        "id": "0PBXNe6aNQi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "KlRIHTfOeEWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Trend or Oscillate"
      ],
      "metadata": {
        "id": "1wZzBFjP5O6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's access the SPY data from your ticker_data dictionary\n",
        "spy_df = ticker_data['SPY (US)'].copy()\n",
        "\n",
        "# Filter to start from the first full market week of 2007\n",
        "# The first trading day of 2007 was January 3rd (Wednesday)\n",
        "# So the first full market week started on January 8th (Monday)\n",
        "start_date = '2007-01-08'\n",
        "spy_df_filtered = spy_df[spy_df.index >= start_date]\n",
        "\n",
        "# Calculate the percent change from open to close\n",
        "spy_df_filtered['pct_change_open_close'] = (spy_df_filtered['SPY_Close'] - spy_df_filtered['SPY_Open']) / spy_df_filtered['SPY_Open'] * 100\n",
        "\n",
        "# Create the target variable oscillate_i\n",
        "# oscillate_i = 1 if absolute percent change > 0.5%, else 0\n",
        "spy_df_filtered['oscillate_i'] = np.where(np.abs(spy_df_filtered['pct_change_open_close']) <= 0.5, 1, 0)\n",
        "\n",
        "# Display some summary statistics\n",
        "print(f\"Date range: {spy_df_filtered.index.min().date()} to {spy_df_filtered.index.max().date()}\")\n",
        "print(f\"Total trading days: {len(spy_df_filtered)}\")\n",
        "print(f\"Days with oscillate_i (oscillate_i = 1): {spy_df_filtered['oscillate_i'].sum()}\")\n",
        "print(f\"Days without oscillate_i (oscillate_i = 0): {len(spy_df_filtered) - spy_df_filtered['oscillate_i'].sum()}\")\n",
        "print(f\"Percentage of oscillate_i days: {spy_df_filtered['oscillate_i'].mean() * 100:.2f}%\")\n",
        "\n",
        "# Let's create a visualization to understand the distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the distribution of daily percentage changes\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(spy_df_filtered['pct_change_open_close'], bins=100, alpha=0.75, edgecolor='black')\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='0.5% threshold')\n",
        "plt.axvline(x=-0.5, color='red', linestyle='--', linewidth=2)\n",
        "plt.xlabel('Daily % Change (Open to Close)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of SPY Daily Percentage Changes (2007-Present)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a small sample output to verify the calculation\n",
        "print(\"\\nSample of the data with oscillate_i:\")\n",
        "sample_data = spy_df_filtered[['SPY_Open', 'SPY_Close', 'pct_change_open_close', 'oscillate_i']].copy()\n",
        "sample_data['abs_pct_change'] = np.abs(sample_data['pct_change_open_close'])\n",
        "print(sample_data.head(10))\n",
        "\n",
        "# Let's also check for balance between trending up and trending down\n",
        "trending_days = spy_df_filtered[spy_df_filtered['oscillate_i'] == 1]\n",
        "trend_up = trending_days[trending_days['pct_change_open_close'] > 0]\n",
        "trend_down = trending_days[trending_days['pct_change_open_close'] < 0]\n",
        "\n",
        "print(f\"\\nTrending days (|change| > 0.5%): {len(trending_days)}\")\n",
        "print(f\"  Upward trends (change > 0.5%): {len(trend_up)}\")\n",
        "print(f\"  Downward trends (change < -0.5%): {len(trend_down)}\")\n",
        "\n",
        "# Let's also check for any outliers or extreme values\n",
        "print(f\"\\nMaximum daily % change: {spy_df_filtered['pct_change_open_close'].max():.2f}%\")\n",
        "print(f\"Minimum daily % change: {spy_df_filtered['pct_change_open_close'].min():.2f}%\")\n",
        "\n",
        "# Create your base DataFrame for feature engineering\n",
        "base_df = spy_df_filtered.copy()\n",
        "print(f\"\\nBase DataFrame shape: {base_df.shape}\")\n",
        "print(f\"Columns: {base_df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "mQfr6ozmcoPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Indice Features"
      ],
      "metadata": {
        "id": "pU_qm4Vk5Wol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's create a list of indices to include (excluding VIX Brazil)\n",
        "indices_to_include = [key for key in ticker_data.keys() if key != 'VIX Brazil']\n",
        "\n",
        "# Let's create a function to extract and rename the relevant columns\n",
        "def extract_columns(df, display_name):\n",
        "    # Extract ticker symbol from the column names\n",
        "    ticker_symbol = df.columns[0].split('_')[0]\n",
        "\n",
        "    # Extract relevant columns and rename them\n",
        "    columns_to_extract = {}\n",
        "\n",
        "    if f'{ticker_symbol}_Open' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Open'] = f'{display_name}_Open'\n",
        "    if f'{ticker_symbol}_High' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_High'] = f'{display_name}_High'\n",
        "    if f'{ticker_symbol}_Low' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Low'] = f'{display_name}_Low'\n",
        "    if f'{ticker_symbol}_Close' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Close'] = f'{display_name}_Close'\n",
        "    if f'{ticker_symbol}_Volume' in df.columns:\n",
        "        columns_to_extract[f'{ticker_symbol}_Volume'] = f'{display_name}_Volume'\n",
        "\n",
        "    # Create a new dataframe with only the relevant columns\n",
        "    extracted_df = df[list(columns_to_extract.keys())].copy()\n",
        "    extracted_df = extracted_df.rename(columns=columns_to_extract)\n",
        "\n",
        "    return extracted_df\n",
        "\n",
        "# Join the data from other indices to the base dataframe\n",
        "for display_name in indices_to_include:\n",
        "    if display_name != 'SPY (US)':  # We already have SPY in the base_df\n",
        "        index_df = ticker_data[display_name]\n",
        "\n",
        "        # Filter to match the date range of base_df\n",
        "        index_df_filtered = index_df[index_df.index >= start_date]\n",
        "\n",
        "        # Extract the relevant columns\n",
        "        extracted_df = extract_columns(index_df_filtered, display_name)\n",
        "\n",
        "        # Join to base_df\n",
        "        base_df = base_df.join(extracted_df, how='left')\n",
        "\n",
        "# Display the resulting dataframe structure\n",
        "print(f\"Base DataFrame shape after joining indices: {base_df.shape}\")\n",
        "print(f\"\\nColumns in base_df:\")\n",
        "for col in base_df.columns:\n",
        "    print(f\"  {col}\")\n",
        "\n",
        "# Check for missing values in the joined data\n",
        "missing_summary = base_df.isnull().sum()\n",
        "if missing_summary.any():\n",
        "    print(\"\\nMissing values in joined data:\")\n",
        "    print(missing_summary[missing_summary > 0])\n",
        "\n",
        "# Sample of the data to verify the join\n",
        "print(\"\\nSample of the joined data:\")\n",
        "sample_columns = ['SPY_Open', 'SPY_High', 'SPY_Low', 'SPY_Close', 'pct_change_open_close', 'oscillate_i']\n",
        "# Add some other index columns to the sample\n",
        "for display_name in indices_to_include[:3]:  # Show first 3 indices as example\n",
        "    if display_name != 'SPY (US)':\n",
        "        open_col = f'{display_name}_Open'\n",
        "        high_col = f'{display_name}_High'\n",
        "        if open_col in base_df.columns and high_col in base_df.columns:\n",
        "            sample_columns.extend([open_col, high_col])\n",
        "\n",
        "print(base_df[sample_columns].head())\n",
        "\n",
        "# Summary of data availability for each index\n",
        "print(\"\\nData availability summary:\")\n",
        "for display_name in indices_to_include:\n",
        "    if display_name != 'SPY (US)':\n",
        "        open_col = f'{display_name}_Open'\n",
        "        high_col = f'{display_name}_High'\n",
        "        low_col = f'{display_name}_Low'\n",
        "        if open_col in base_df.columns:\n",
        "            non_null_count = base_df[open_col].count()\n",
        "            total_rows = len(base_df)\n",
        "            coverage = (non_null_count / total_rows) * 100\n",
        "\n",
        "            # Check if we have High and Low columns\n",
        "            has_high = high_col in base_df.columns\n",
        "            has_low = low_col in base_df.columns\n",
        "\n",
        "            print(f\"{display_name}: {non_null_count}/{total_rows} ({coverage:.1f}% coverage)\")\n",
        "            print(f\"  Has High: {has_high}, Has Low: {has_low}\")"
      ],
      "metadata": {
        "id": "UyJ9eUyKemft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_all_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Calculate technical indicators for all columns in the dataframe.\n",
        "    Returns a new dataframe with the original data plus technical indicators.\n",
        "    \"\"\"\n",
        "    df_with_indicators = df.copy()\n",
        "\n",
        "    # Get all unique ticker names from the columns\n",
        "    tickers = set()\n",
        "    for col in df.columns:\n",
        "        if '_' in col:\n",
        "            ticker = col.rsplit('_', 1)[0]  # Get everything before the last underscore\n",
        "            tickers.add(ticker)\n",
        "\n",
        "    # Remove some non-ticker columns that might have been picked up\n",
        "    tickers -= {'pct_change_open', 'trend'}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        # Check if we have the necessary columns for technical analysis\n",
        "        has_ohlc = all([\n",
        "            f'{ticker}_Open' in df.columns,\n",
        "            f'{ticker}_High' in df.columns,\n",
        "            f'{ticker}_Low' in df.columns,\n",
        "            f'{ticker}_Close' in df.columns\n",
        "        ])\n",
        "\n",
        "        if has_ohlc:\n",
        "            # RSI (14-day)\n",
        "            df_with_indicators[f'{ticker}_RSI_14'] = ta.momentum.RSIIndicator(\n",
        "                close=df[f'{ticker}_Close'],\n",
        "                window=14\n",
        "            ).rsi()\n",
        "\n",
        "            # ATR (14-day)\n",
        "            df_with_indicators[f'{ticker}_ATR_14'] = ta.volatility.AverageTrueRange(\n",
        "                high=df[f'{ticker}_High'],\n",
        "                low=df[f'{ticker}_Low'],\n",
        "                close=df[f'{ticker}_Close'],\n",
        "                window=14\n",
        "            ).average_true_range()\n",
        "\n",
        "            # Bollinger Bands\n",
        "            bollinger = ta.volatility.BollingerBands(\n",
        "                close=df[f'{ticker}_Close'],\n",
        "                window=20,\n",
        "                window_dev=2\n",
        "            )\n",
        "            df_with_indicators[f'{ticker}_BB_Upper'] = bollinger.bollinger_hband()\n",
        "            df_with_indicators[f'{ticker}_BB_Lower'] = bollinger.bollinger_lband()\n",
        "            df_with_indicators[f'{ticker}_BB_Middle'] = bollinger.bollinger_mavg()\n",
        "\n",
        "            # MACD\n",
        "            macd = ta.trend.MACD(close=df[f'{ticker}_Close'])\n",
        "            df_with_indicators[f'{ticker}_MACD'] = macd.macd()\n",
        "            df_with_indicators[f'{ticker}_MACD_Signal'] = macd.macd_signal()\n",
        "            df_with_indicators[f'{ticker}_MACD_Diff'] = macd.macd_diff()\n",
        "\n",
        "            # Moving Averages\n",
        "            df_with_indicators[f'{ticker}_SMA_20'] = ta.trend.SMAIndicator(\n",
        "                close=df[f'{ticker}_Close'],\n",
        "                window=20\n",
        "            ).sma_indicator()\n",
        "\n",
        "            df_with_indicators[f'{ticker}_EMA_20'] = ta.trend.EMAIndicator(\n",
        "                close=df[f'{ticker}_Close'],\n",
        "                window=20\n",
        "            ).ema_indicator()\n",
        "\n",
        "            # Momentum\n",
        "            df_with_indicators[f'{ticker}_Momentum_10'] = ta.momentum.ROCIndicator(\n",
        "                close=df[f'{ticker}_Close'],\n",
        "                window=10\n",
        "            ).roc()\n",
        "\n",
        "            # Stochastic Oscillator\n",
        "            stoch = ta.momentum.StochasticOscillator(\n",
        "                high=df[f'{ticker}_High'],\n",
        "                low=df[f'{ticker}_Low'],\n",
        "                close=df[f'{ticker}_Close'],\n",
        "                window=14,\n",
        "                smooth_window=3\n",
        "            )\n",
        "            df_with_indicators[f'{ticker}_Stoch_K'] = stoch.stoch()\n",
        "            df_with_indicators[f'{ticker}_Stoch_D'] = stoch.stoch_signal()\n",
        "\n",
        "            # For volume-based indicators (if volume exists)\n",
        "            if f'{ticker}_Volume' in df.columns:\n",
        "                # On-Balance Volume\n",
        "                df_with_indicators[f'{ticker}_OBV'] = ta.volume.OnBalanceVolumeIndicator(\n",
        "                    close=df[f'{ticker}_Close'],\n",
        "                    volume=df[f'{ticker}_Volume']\n",
        "                ).on_balance_volume()\n",
        "\n",
        "                # Money Flow Index\n",
        "                df_with_indicators[f'{ticker}_MFI'] = ta.volume.MFIIndicator(\n",
        "                    high=df[f'{ticker}_High'],\n",
        "                    low=df[f'{ticker}_Low'],\n",
        "                    close=df[f'{ticker}_Close'],\n",
        "                    volume=df[f'{ticker}_Volume'],\n",
        "                    window=14\n",
        "                ).money_flow_index()\n",
        "\n",
        "    return df_with_indicators\n",
        "\n",
        "# Apply the function to your data\n",
        "base_df_with_indicators = calculate_all_technical_indicators(base_df)\n",
        "\n",
        "# Check what indicators were created\n",
        "print(\"Technical indicators created for each ticker:\")\n",
        "new_cols = [col for col in base_df_with_indicators.columns if col not in base_df.columns]\n",
        "print(f\"Total new columns: {len(new_cols)}\\n\")\n",
        "\n",
        "# Show indicators by ticker\n",
        "for ticker in sorted(set([col.rsplit('_', 2)[0] for col in new_cols])):\n",
        "    ticker_indicators = [col for col in new_cols if col.startswith(ticker)]\n",
        "    if ticker_indicators:\n",
        "        print(f\"{ticker}:\")\n",
        "        for col in ticker_indicators:\n",
        "            print(f\"  - {col}\")\n",
        "        print()\n",
        "\n",
        "# Verify data integrity\n",
        "missing_summary = base_df_with_indicators.isnull().sum()\n",
        "if missing_summary.any():\n",
        "    print(\"\\nWarning: Some indicators have missing values\")\n",
        "    missing_indicators = missing_summary[missing_summary > 0]\n",
        "    print(f\"Total indicators with missing values: {len(missing_indicators)}\")\n",
        "\n",
        "    # Note: Technical indicators typically have some missing values at the beginning\n",
        "    # due to their calculation windows (e.g., 14-day RSI will have 13 missing values)\n",
        "    print(\"\\nMissing values per indicator (first few):\")\n",
        "    for col in missing_indicators.index[:10]:\n",
        "        print(f\"  {col}: {missing_indicators[col]} missing values\")"
      ],
      "metadata": {
        "id": "siEZe-BwzO6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_df_with_indicators.columns"
      ],
      "metadata": {
        "id": "LxCN0X90zuY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Calendar Date Features"
      ],
      "metadata": {
        "id": "enbDTgnWaBaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_event_indicators(base_df, calendar_df):\n",
        "    \"\"\"\n",
        "    Create binary indicators for economic events with clearer naming convention\n",
        "    \"\"\"\n",
        "    # Ensure calendar_df has datetime index\n",
        "    calendar_df['date'] = pd.to_datetime(calendar_df['date'], format='%d/%m/%Y')\n",
        "\n",
        "    # Create indicators initialized to 0\n",
        "    base_df['cpi'] = 0\n",
        "    base_df['employment'] = 0\n",
        "    base_df['fed_meeting'] = 0\n",
        "    base_df['fed_proj'] = 0\n",
        "\n",
        "    # Process each event in the calendar\n",
        "    for _, event in calendar_df.iterrows():\n",
        "        event_date = event['date'].date()\n",
        "        event_name = event['event']\n",
        "\n",
        "        # Skip if the date is not in our base dataframe\n",
        "        if event_date not in base_df.index.date:\n",
        "            continue\n",
        "\n",
        "        # Find the matching index in base_df\n",
        "        matching_dates = base_df.index.date == event_date\n",
        "        if not any(matching_dates):\n",
        "            continue\n",
        "\n",
        "        date_idx = base_df.index[matching_dates][0]\n",
        "\n",
        "        # CPI Indicator\n",
        "        if ('CPI (MoM)' in event_name or 'CPI (YoY)' in event_name) and 'Core' not in event_name:\n",
        "            base_df.loc[date_idx, 'cpi'] = 1\n",
        "\n",
        "        # Employment Indicator\n",
        "        elif 'Nonfarm Payrolls' in event_name or 'Unemployment Rate' in event_name:\n",
        "            base_df.loc[date_idx, 'employment'] = 1\n",
        "\n",
        "        # Fed Meeting Indicator\n",
        "        elif 'Fed Interest Rate Decision' in event_name or \\\n",
        "             'FOMC Statement' in event_name or \\\n",
        "             'FOMC Meeting Minutes' in event_name:\n",
        "            base_df.loc[date_idx, 'fed_meeting'] = 1\n",
        "\n",
        "        # Fed Projections Indicator\n",
        "        elif 'FOMC Economic Projections' in event_name:\n",
        "            base_df.loc[date_idx, 'fed_proj'] = 1\n",
        "            # Also set fed_meeting since projections occur during Fed meetings\n",
        "            base_df.loc[date_idx, 'fed_meeting'] = 1\n",
        "\n",
        "    # Create lagged and lead versions with clearer naming\n",
        "    base_df['fed_proj_lag2'] = base_df['fed_proj'].shift(-2)\n",
        "    base_df['fed_proj_lag1'] = base_df['fed_proj'].shift(1)\n",
        "    base_df['fed_proj_lead1'] = base_df['fed_proj'].shift(-1)\n",
        "\n",
        "    base_df['fed_meeting_lag2'] = base_df['fed_meeting'].shift(-2)\n",
        "    base_df['fed_meeting_lag1'] = base_df['fed_meeting'].shift(1)\n",
        "    base_df['fed_meeting_lead1'] = base_df['fed_meeting'].shift(-1)\n",
        "\n",
        "    base_df['cpi_lag2'] = base_df['cpi'].shift(-2)\n",
        "    base_df['cpi_lag1'] = base_df['cpi'].shift(1)\n",
        "    base_df['cpi_lead1'] = base_df['cpi'].shift(-1)\n",
        "\n",
        "    base_df['employment_lag2'] = base_df['employment'].shift(-2)\n",
        "    base_df['employment_lag1'] = base_df['employment'].shift(1)\n",
        "    base_df['employment_lead1'] = base_df['employment'].shift(-1)\n",
        "\n",
        "    return base_df\n",
        "\n",
        "# Apply the function\n",
        "base_df_with_indicators = create_event_indicators(base_df_with_indicators, calendar_df)\n",
        "\n",
        "# Verify the indicators were created\n",
        "print(\"Event indicators summary:\")\n",
        "print(f\"Days with CPI releases: {base_df_with_indicators['cpi'].sum()}\")\n",
        "print(f\"Days with Employment releases: {base_df_with_indicators['employment'].sum()}\")\n",
        "print(f\"Days with Fed meetings: {base_df_with_indicators['fed_meeting'].sum()}\")\n",
        "print(f\"Days with Fed projections: {base_df_with_indicators['fed_proj'].sum()}\")\n",
        "\n",
        "# Let's verify by checking specific dates around a known Fed projection event\n",
        "# For the 2023-12-13 FOMC Economic Projections\n",
        "test_dates = pd.date_range(start='2023-12-11', end='2023-12-15')\n",
        "test_dates = [d for d in test_dates if d in base_df_with_indicators.index]\n",
        "\n",
        "if test_dates:\n",
        "    test_df = base_df_with_indicators.loc[test_dates]\n",
        "    print(\"\\nFed projection indicators around December 13, 2023:\")\n",
        "    print(test_df[['fed_proj', 'fed_proj_lag1', 'fed_proj_lag2', 'fed_proj_lead1']])\n",
        "else:\n",
        "    print(\"\\nNo matching dates found in base_df for the test period\")"
      ],
      "metadata": {
        "id": "UyELmDX6YIou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asian_markets = ['Nikkei 225 (Japan)', 'Hang Seng (Hong Kong)', 'SSE Composite (China)', 'ASX 200 (Australia)']\n",
        "european_markets = ['DAX (Germany)', 'FTSE 100 (UK)', 'CAC 40 (France)', 'Euro Stoxx 50 (EU)']\n",
        "us_markets = ['SPY']\n",
        "currency_pairs = ['EUR/USD', 'JPY/USD', 'CNY/USD']\n",
        "commodities = ['Gold', 'Crude Oil', 'Silver', 'Corn', 'Copper']\n",
        "volatility_indices = ['VIX (US)', 'US Dollar Index']\n",
        "\n",
        "event_indicators = [\n",
        "    # Base indicators\n",
        "    'cpi', 'employment', 'fed_meeting', 'fed_proj',\n",
        "\n",
        "    # Lagged indicators (past events)\n",
        "    'cpi_lag1', 'employment_lag1', 'fed_meeting_lag1', 'fed_proj_lag1',\n",
        "    'cpi_lag2', 'employment_lag2', 'fed_meeting_lag2', 'fed_proj_lag2',\n",
        "\n",
        "    # Lead indicators (future events)\n",
        "    'cpi_lead1', 'employment_lead1', 'fed_meeting_lead1', 'fed_proj_lead1'\n",
        "]\n",
        "\n",
        "def create_training_dataframe(base_df, base_df_with_indicators, fred_data=None, fred_frequencies=None):\n",
        "    \"\"\"\n",
        "    Create a clean dataframe for model training with specific requirements for each market type\n",
        "    Also handles FRED data with appropriate lags for 10 AM Eastern predictions\n",
        "    \"\"\"\n",
        "    training_df = pd.DataFrame(index=base_df_with_indicators.index)\n",
        "\n",
        "    # Define market groups\n",
        "    asian_markets = ['Nikkei 225 (Japan)', 'Hang Seng (Hong Kong)', 'SSE Composite (China)', 'ASX 200 (Australia)']\n",
        "    european_markets = ['DAX (Germany)', 'FTSE 100 (UK)', 'CAC 40 (France)', 'Euro Stoxx 50 (EU)']\n",
        "    us_markets = ['SPY']\n",
        "    currency_pairs = ['EUR/USD', 'JPY/USD', 'CNY/USD']\n",
        "    commodities = ['Gold', 'Crude Oil', 'Silver', 'Corn', 'Copper']\n",
        "    volatility_indices = ['VIX (US)', 'US Dollar Index']\n",
        "\n",
        "    # Define technical indicators to include\n",
        "    technical_indicators = [\n",
        "        'RSI_14', 'ATR_14', 'BB_Upper', 'BB_Lower', 'BB_Middle',\n",
        "        'MACD', 'MACD_Signal', 'MACD_Diff', 'SMA_20', 'EMA_20',\n",
        "        'Momentum_10', 'Stoch_K', 'Stoch_D', 'OBV', 'MFI'\n",
        "    ]\n",
        "\n",
        "    # 1. Asian markets - technical indicators with no lag\n",
        "    for market in asian_markets:\n",
        "        for indicator in technical_indicators:\n",
        "            col_name = f'{market}_{indicator}'\n",
        "            if col_name in base_df_with_indicators.columns:\n",
        "                training_df[f'{market}_{indicator}_current'] = base_df_with_indicators[col_name]\n",
        "\n",
        "    # 2. European markets - lag 1 technical indicators + current day open\n",
        "    for market in european_markets:\n",
        "        # Lag 1 technical indicators\n",
        "        for indicator in technical_indicators:\n",
        "            col_name = f'{market}_{indicator}'\n",
        "            if col_name in base_df_with_indicators.columns:\n",
        "                training_df[f'{market}_{indicator}_lag1'] = base_df_with_indicators[col_name].shift(1)\n",
        "\n",
        "        # Current day open\n",
        "        if f'{market}_Open' in base_df.columns:\n",
        "            training_df[f'{market}_Open_current'] = base_df[f'{market}_Open']\n",
        "\n",
        "    # 3. SPY - lag 1 technical indicators + current day open + lag 1 raw values\n",
        "    # Lag 1 technical indicators\n",
        "    for indicator in technical_indicators:\n",
        "        col_name = f'SPY_{indicator}'\n",
        "        if col_name in base_df_with_indicators.columns:\n",
        "            training_df[f'SPY_{indicator}_lag1'] = base_df_with_indicators[col_name].shift(1)\n",
        "\n",
        "    # Current day open\n",
        "    if 'SPY_Open' in base_df.columns:\n",
        "        training_df['SPY_Open_current'] = base_df['SPY_Open']\n",
        "\n",
        "    # Lag 1 raw values for SPY\n",
        "    spy_raw_columns = ['SPY_Close', 'SPY_High', 'SPY_Low', 'SPY_Volume']\n",
        "    for col in spy_raw_columns:\n",
        "        if col in base_df.columns:\n",
        "            training_df[f'{col}_lag1'] = base_df[col].shift(1)\n",
        "\n",
        "    # 4. Currency pairs, commodities, volatility indices - lag 1 technical indicators + current day open\n",
        "    combined_markets = currency_pairs + commodities + volatility_indices\n",
        "\n",
        "    for market in combined_markets:\n",
        "        # Lag 1 technical indicators\n",
        "        for indicator in technical_indicators:\n",
        "            col_name = f'{market}_{indicator}'\n",
        "            if col_name in base_df_with_indicators.columns:\n",
        "                training_df[f'{market}_{indicator}_lag1'] = base_df_with_indicators[col_name].shift(1)\n",
        "\n",
        "        # Current day open\n",
        "        if f'{market}_Open' in base_df.columns:\n",
        "            training_df[f'{market}_Open_current'] = base_df[f'{market}_Open']\n",
        "\n",
        "    # 5. Add economic event indicators\n",
        "    event_indicators = [\n",
        "    # Base indicators\n",
        "    'cpi', 'employment', 'fed_meeting', 'fed_proj',\n",
        "\n",
        "    # Lagged indicators (past events)\n",
        "    'cpi_lag1', 'employment_lag1', 'fed_meeting_lag1', 'fed_proj_lag1',\n",
        "    'cpi_lag2', 'employment_lag2', 'fed_meeting_lag2', 'fed_proj_lag2',\n",
        "\n",
        "    # Lead indicators (future events)\n",
        "    'cpi_lead1', 'employment_lead1', 'fed_meeting_lead1', 'fed_proj_lead1'\n",
        "]\n",
        "\n",
        "    for indicator in event_indicators:\n",
        "        if indicator in base_df_with_indicators.columns:\n",
        "            training_df[indicator] = base_df_with_indicators[indicator]\n",
        "\n",
        "    # 6. Process FRED data if provided\n",
        "    if fred_data is not None and fred_frequencies is not None:\n",
        "        # Define series by their release timing\n",
        "        daily_series = ['DFF', 'T10Y2Y', 'GS5', 'GS30']  # Usually updated after market close\n",
        "        weekly_series = ['STLFSI', 'BAMLH0A0HYM2', 'BAMLC0A0CM']  # Updated on specific days\n",
        "        monthly_series = ['CPIAUCSL', 'UNRATE', 'M2SL', 'USSLIND']  # Fixed monthly releases\n",
        "\n",
        "        # Create a date range matching base_df\n",
        "        date_range = pd.date_range(start=base_df.index.min(), end=base_df.index.max(), freq='D')\n",
        "\n",
        "        for series_id, df in fred_data.items():\n",
        "            value_col = df.columns[0]\n",
        "            freq = fred_frequencies[series_id]\n",
        "\n",
        "            # Reindex to daily frequency first\n",
        "            df_daily = df.reindex(date_range)\n",
        "\n",
        "            if series_id in daily_series:\n",
        "                # For daily data, use 1-day lag to ensure availability at 10 AM\n",
        "                df_daily[value_col] = df_daily[value_col].ffill().shift(1)\n",
        "\n",
        "            elif series_id in weekly_series:\n",
        "                # For weekly data, handle based on release schedule\n",
        "                if series_id == 'STLFSI':  # St. Louis Fed Financial Stress Index updates Thursday\n",
        "                    # Use 4-day lag to ensure we have data at 10 AM Monday\n",
        "                    df_daily[value_col] = df_daily[value_col].ffill().shift(4)\n",
        "                else:\n",
        "                    # Other weekly series: use 1-day lag\n",
        "                    df_daily[value_col] = df_daily[value_col].ffill().shift(1)\n",
        "\n",
        "            elif series_id in monthly_series:\n",
        "                # For monthly data, forward fill and apply appropriate lag\n",
        "                df_daily[value_col] = df_daily[value_col].ffill()\n",
        "\n",
        "                if series_id == 'CPIAUCSL':  # CPI releases around the 10th-15th\n",
        "                    df_daily[value_col] = df_daily[value_col].shift(15)\n",
        "                elif series_id == 'UNRATE':  # Unemployment rate releases first Friday\n",
        "                    df_daily[value_col] = df_daily[value_col].shift(10)\n",
        "                else:  # M2 and Leading Index\n",
        "                    df_daily[value_col] = df_daily[value_col].shift(20)\n",
        "\n",
        "            # Merge with training_df\n",
        "            training_df[f'{series_id}_10am'] = df_daily[value_col]\n",
        "\n",
        "            # Add derived features for FRED data\n",
        "            if series_id == 'DFF':\n",
        "                training_df['fed_funds_change'] = training_df[f'{series_id}_10am'].pct_change()\n",
        "            elif series_id == 'T10Y2Y':\n",
        "                training_df['yield_curve_change'] = training_df[f'{series_id}_10am'].diff()\n",
        "            elif series_id == 'STLFSI':\n",
        "                training_df['financial_stress_change'] = training_df[f'{series_id}_10am'].diff()\n",
        "            elif series_id == 'CPIAUCSL':\n",
        "                training_df['cpi_mom'] = training_df[f'{series_id}_10am'].pct_change()\n",
        "            elif series_id == 'UNRATE':\n",
        "                training_df['unemployment_change'] = training_df[f'{series_id}_10am'].diff()\n",
        "\n",
        "        # Add interaction features for FRED data\n",
        "        if 'T10Y2Y_10am' in training_df.columns and 'STLFSI_10am' in training_df.columns:\n",
        "            training_df['yield_curve_stress'] = training_df['T10Y2Y_10am'] * training_df['STLFSI_10am']\n",
        "\n",
        "    # 7. Add target variable\n",
        "    if 'oscillate_i' in base_df.columns:\n",
        "        training_df['oscillate_i_current'] = base_df['oscillate_i']\n",
        "\n",
        "    # Drop the first 33 rows to account for:\n",
        "    # - SMA/EMA 20-day calculations (20 days)\n",
        "    # - RSI/ATR 14-day calculations (14 days)\n",
        "    # - MACD calculations (26 days for standard MACD)\n",
        "    # - 1 additional row for lag operations\n",
        "    # - A few extra rows for safety\n",
        "    training_df = training_df.iloc[33:].copy()\n",
        "\n",
        "    # Handle any remaining NaN values at the beginning of the series for FRED data\n",
        "    for col in training_df.columns:\n",
        "        if '_10am' in col:\n",
        "            # For the beginning of the series where we don't have enough history\n",
        "            training_df[col] = training_df[col].bfill()\n",
        "\n",
        "    # Verify no null values remain\n",
        "    null_count = training_df.isnull().sum().sum()\n",
        "    print(f\"Total null values after dropping first 33 rows: {null_count}\")\n",
        "\n",
        "    return training_df\n",
        "\n",
        "# Create the training dataframe with FRED data and event indicators\n",
        "training_df = create_training_dataframe(base_df, base_df_with_indicators, fred_data[0], fred_data[1])\n",
        "\n",
        "# Display structure and verify\n",
        "print(f\"Training dataframe shape: {training_df.shape}\")\n",
        "print(\"\\nColumn types by market group:\")\n",
        "\n",
        "# Count columns by type\n",
        "asian_cols = [col for col in training_df.columns if any(market in col for market in asian_markets)]\n",
        "euro_cols = [col for col in training_df.columns if any(market in col for market in european_markets)]\n",
        "spy_cols = [col for col in training_df.columns if 'SPY' in col]\n",
        "currency_cols = [col for col in training_df.columns if any(pair in col for pair in currency_pairs)]\n",
        "commodity_cols = [col for col in training_df.columns if any(comm in col for comm in commodities)]\n",
        "volatility_cols = [col for col in training_df.columns if any(vol in col for vol in volatility_indices)]\n",
        "fred_cols = [col for col in training_df.columns if '_10am' in col or col in ['fed_funds_change', 'yield_curve_change', 'financial_stress_change', 'cpi_mom', 'unemployment_change', 'yield_curve_stress']]\n",
        "event_cols = [col for col in training_df.columns if col in event_indicators]\n",
        "\n",
        "print(f\"Asian market columns: {len(asian_cols)}\")\n",
        "print(f\"European market columns: {len(euro_cols)}\")\n",
        "print(f\"SPY columns: {len(spy_cols)}\")\n",
        "print(f\"Currency pair columns: {len(currency_cols)}\")\n",
        "print(f\"Commodity columns: {len(commodity_cols)}\")\n",
        "print(f\"Volatility index columns: {len(volatility_cols)}\")\n",
        "print(f\"FRED economic indicator columns: {len(fred_cols)}\")\n",
        "print(f\"Economic event indicator columns: {len(event_cols)}\")\n",
        "\n",
        "# Final check for any remaining missing values\n",
        "missing_summary = training_df.isnull().sum()\n",
        "if missing_summary.any():\n",
        "    print(\"\\nWarning: Some columns still have missing values\")\n",
        "    missing_df = pd.DataFrame({'Missing_Count': missing_summary[missing_summary > 0]})\n",
        "    missing_df['Percentage'] = (missing_df['Missing_Count'] / len(training_df) * 100).round(2)\n",
        "    print(f\"Total columns with missing values: {len(missing_df)}\")\n",
        "\n",
        "    # Find columns with more than 50% missing values\n",
        "    high_missing_cols = missing_df[missing_df['Percentage'] > 50].index.tolist()\n",
        "\n",
        "    if high_missing_cols:\n",
        "        print(f\"\\nDropping {len(high_missing_cols)} columns with >50% missing values:\")\n",
        "        for col in high_missing_cols:\n",
        "            print(f\"  - {col}: {missing_df.loc[col, 'Percentage']}% missing\")\n",
        "\n",
        "        # Drop these columns\n",
        "        training_df = training_df.drop(columns=high_missing_cols)\n",
        "\n",
        "        # Recalculate missing summary after dropping columns\n",
        "        missing_summary = training_df.isnull().sum()\n",
        "        missing_df = pd.DataFrame({'Missing_Count': missing_summary[missing_summary > 0]})\n",
        "        missing_df['Percentage'] = (missing_df['Missing_Count'] / len(training_df) * 100).round(2)\n",
        "\n",
        "    print(\"\\nRemaining columns with missing values (if any):\")\n",
        "    if len(missing_df) > 0:\n",
        "        print(missing_df.head(10))\n",
        "    else:\n",
        "        print(\"No more columns with missing values!\")\n",
        "else:\n",
        "    print(\"\\nSuccess: No missing values in the training dataframe!\")\n",
        "\n",
        "# Sample of the final dataframe\n",
        "print(\"\\nFirst 5 rows of training dataframe:\")\n",
        "print(training_df.head())\n",
        "\n",
        "# Print final shape\n",
        "print(f\"\\nFinal training dataframe shape: {training_df.shape}\")"
      ],
      "metadata": {
        "id": "7iCXdfrS3k9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development"
      ],
      "metadata": {
        "id": "6l5IjaqLeHHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Test split and building Evaluation Function"
      ],
      "metadata": {
        "id": "qw3A5WIb5mbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Filter data to keep only 2007-2024 (drop 2025)\n",
        "# Filter out 2025 data\n",
        "base_df_filtered = training_df[training_df.index.year < 2025].copy()\n",
        "\n",
        "# Split into train (2007-2023) and test (2024)\n",
        "train_data = base_df_filtered[base_df_filtered.index.year < 2024]\n",
        "test_data = base_df_filtered[base_df_filtered.index.year == 2024]\n",
        "\n",
        "print(f\"Train data period: {train_data.index.min().date()} to {train_data.index.max().date()}\")\n",
        "print(f\"Test data period: {test_data.index.min().date()} to {test_data.index.max().date()}\")\n",
        "print(f\"Train shape: {train_data.shape}, Test shape: {test_data.shape}\")\n"
      ],
      "metadata": {
        "id": "yxW30g14EBDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Prepare the data for modeling by handling missing values and creating X, y\n",
        "    \"\"\"\n",
        "    # Separate features and target for train\n",
        "    X_train = train_df.drop('oscillate_i_current', axis=1)\n",
        "    y_train = train_df['oscillate_i_current']\n",
        "\n",
        "    # Separate features and target for test\n",
        "    X_test = test_df.drop('oscillate_i_current', axis=1)\n",
        "    y_test = test_df['oscillate_i_current']\n",
        "\n",
        "    # Handle missing values using imputer fitted on training data\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train),\n",
        "                                   columns=X_train.columns,\n",
        "                                   index=X_train.index)\n",
        "    X_test_imputed = pd.DataFrame(imputer.transform(X_test),\n",
        "                                  columns=X_test.columns,\n",
        "                                  index=X_test.index)\n",
        "\n",
        "    # Scale features using scaler fitted on training data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed),\n",
        "                                  columns=X_train.columns,\n",
        "                                  index=X_train.index)\n",
        "    X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed),\n",
        "                                 columns=X_test.columns,\n",
        "                                 index=X_test.index)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, imputer"
      ],
      "metadata": {
        "id": "pre1FqVdCncf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, scaler, imputer = prepare_data(train_data, test_data)\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"y_test distribution:\\n{y_test.value_counts()}\")"
      ],
      "metadata": {
        "id": "mmhqhoyHCnfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate model performance and visualize results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{model_name} Performance on 2024 Test Data:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "    # Calculate naive predictor accuracy\n",
        "    naive_accuracy = max(y_true.mean(), 1 - y_true.mean())\n",
        "    improvement = (accuracy_score(y_true, y_pred) - naive_accuracy) * 100\n",
        "    print(f\"Naive Predictor Accuracy: {naive_accuracy:.4f}\")\n",
        "    print(f\"Improvement over Naive: {improvement:.2f}%\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{model_name} Confusion Matrix - 2024 Test Data')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    # Return metrics for comparison\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'improvement': improvement,\n",
        "        'confusion_matrix': cm,\n",
        "        'classification_report': classification_report(y_true, y_pred, output_dict=True)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "6YSkDTTCCnh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "PT6bprO-NwI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def select_important_features(X_train, y_train, X_test,\n",
        "                              num_features=30, random_state=42, oversample=True):\n",
        "    \"\"\"\n",
        "    1) Optionally SMOTEupsample the minority class in X_train/y_train\n",
        "    2) Fit RandomForest to determine top `num_features`\n",
        "    3) Return the **oversampled** & **columnfiltered** X_train plus filtered X_test\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train_filtered : pd.DataFrame  # may have more rows if oversampled\n",
        "    X_test_filtered  : pd.DataFrame\n",
        "    y_train_resampled: pd.Series     # aligned with X_train_filtered\n",
        "    \"\"\"\n",
        "    # 1) Copy and optionally SMOTE\n",
        "    X_tr, y_tr = X_train.copy(), y_train.copy()\n",
        "    if oversample:\n",
        "        sm = SMOTE(random_state=random_state)\n",
        "        X_res, y_res = sm.fit_resample(X_tr, y_tr)\n",
        "        # wrap back into DataFrame\n",
        "        X_tr = pd.DataFrame(X_res, columns=X_train.columns)\n",
        "        y_tr = pd.Series(y_res, name=y_train.name)\n",
        "\n",
        "    # 2) Train RF on (possibly oversampled) data\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100, random_state=random_state, n_jobs=-1\n",
        "    )\n",
        "    model.fit(X_tr, y_tr)\n",
        "\n",
        "    # 3) Pick top features\n",
        "    importances = model.feature_importances_\n",
        "    top_idx     = np.argsort(importances)[::-1][:num_features]\n",
        "    top_cols    = X_train.columns[top_idx]\n",
        "\n",
        "    # 4) Subset **oversampled** train and original test\n",
        "    X_train_filtered = X_tr[top_cols].reset_index(drop=True)\n",
        "    X_test_filtered  = X_test[top_cols].copy()\n",
        "\n",
        "    return X_train_filtered, X_test_filtered, y_tr.reset_index(drop=True)\n",
        "\n",
        "\n",
        "X_train_filt, X_test_filt, y_train_resampled  = select_important_features(\n",
        "    X_train, y_train, X_test, num_features=150\n",
        ")\n",
        "print(\"Filtered shapes:\", X_train_filt.shape, X_test_filt.shape)\n"
      ],
      "metadata": {
        "id": "5hpSRTR_O_ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building XGBoost Classifier"
      ],
      "metadata": {
        "id": "KE2zetlV5s9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_xgboost(X_train, y_train, X_test, n_splits=5):\n",
        "    \"\"\"\n",
        "    Train XGBoost using time series cross-validation for validation,\n",
        "    then train final model on all training data\n",
        "    \"\"\"\n",
        "    # Use TimeSeriesSplit on training data for validation\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    cv_scores = []\n",
        "\n",
        "    # Validate model using time series CV\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
        "        X_train_fold = X_train.iloc[train_idx]\n",
        "        X_val_fold = X_train.iloc[val_idx]\n",
        "        y_train_fold = y_train.iloc[train_idx]\n",
        "        y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "        # Create and train model for this fold\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Validate on fold\n",
        "        y_pred_val = model.predict(X_val_fold)\n",
        "        fold_score = accuracy_score(y_val_fold, y_pred_val)\n",
        "        cv_scores.append(fold_score)\n",
        "\n",
        "        print(f\"Fold {fold + 1}/{n_splits} validation accuracy: {fold_score:.4f}\")\n",
        "\n",
        "    print(f\"Average CV accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Train final model on all training data\n",
        "    final_model = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = final_model.predict(X_test)\n",
        "\n",
        "    return y_pred, final_model, cv_scores\n",
        "\n",
        "# Train and evaluate XGBoost\n",
        "y_pred_xgb, model_xgb, xgb_cv_scores = train_xgboost(X_train_filt, y_train_resampled, X_test_filt)\n",
        "xgb_results = evaluate_model(y_test, y_pred_xgb, 'XGBoost')"
      ],
      "metadata": {
        "id": "ClUv3UEFDZMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Logistic Regression"
      ],
      "metadata": {
        "id": "KN3nS1ZR5jqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_logistic_regression(X_train, y_train, X_test, n_splits=5):\n",
        "    \"\"\"\n",
        "    Train Logistic Regression using time series cross-validation for validation,\n",
        "    then train final model on all training data\n",
        "    \"\"\"\n",
        "    # Use TimeSeriesSplit on training data for validation\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    cv_scores = []\n",
        "\n",
        "    # Validate model using time series CV\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
        "        X_train_fold = X_train.iloc[train_idx]\n",
        "        X_val_fold = X_train.iloc[val_idx]\n",
        "        y_train_fold = y_train.iloc[train_idx]\n",
        "        y_val_fold = y_train.iloc[val_idx]\n",
        "\n",
        "        # Create and train model for this fold\n",
        "        model = LogisticRegression(\n",
        "            random_state=42,\n",
        "            max_iter=1000,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Validate on fold\n",
        "        y_pred_val = model.predict(X_val_fold)\n",
        "        fold_score = accuracy_score(y_val_fold, y_pred_val)\n",
        "        cv_scores.append(fold_score)\n",
        "\n",
        "        print(f\"Fold {fold + 1}/{n_splits} validation accuracy: {fold_score:.4f}\")\n",
        "\n",
        "    print(f\"Average CV accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "    # Train final model on all training data\n",
        "    final_model = LogisticRegression(\n",
        "        random_state=42,\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on test data\n",
        "    y_pred = final_model.predict(X_test)\n",
        "\n",
        "    return y_pred, final_model, cv_scores\n",
        "\n",
        "# Train and evaluate Logistic Regression\n",
        "y_pred_lr, model_lr, lr_cv_scores = train_logistic_regression(X_train_filt, y_train_resampled, X_test_filt)\n",
        "lr_results = evaluate_model(y_test, y_pred_lr, 'Logistic Regression')\n"
      ],
      "metadata": {
        "id": "mOqv8MyNCnkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build LSTM Neural Network"
      ],
      "metadata": {
        "id": "ZTpibcQy59Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  # Add this import\n",
        "\n",
        "def build_lstm_model(hp):\n",
        "    \"\"\"\n",
        "    Create an LSTM model with hyperparameters to tune\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        Input(shape=(1, X_train_filt.shape[1])),\n",
        "\n",
        "        # LSTM layers with tunable units\n",
        "        LSTM(hp.Int('lstm_units_1', min_value=32, max_value=96, step=32),\n",
        "             return_sequences=True),\n",
        "        Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.5, step=0.1)),\n",
        "\n",
        "        LSTM(hp.Int('lstm_units_2', min_value=16, max_value=48, step=16)),\n",
        "        Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.5, step=0.1)),\n",
        "\n",
        "        # Dense layer with tunable units\n",
        "        Dense(hp.Int('dense_units', min_value=8, max_value=32, step=8),\n",
        "              activation='relu'),\n",
        "        Dropout(hp.Float('dropout_3', min_value=0.2, max_value=0.5, step=0.1)),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile with tunable learning rate\n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.Float('learning_rate',\n",
        "                               min_value=1e-4,\n",
        "                               max_value=1e-2,\n",
        "                               sampling='LOG')),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def tune_lstm_hyperparameters(X_train, y_train, max_trials=10, epochs=20, batch_size=32):\n",
        "    \"\"\"\n",
        "    Tune LSTM hyperparameters using Keras Tuner\n",
        "    \"\"\"\n",
        "    # Reshape data for LSTM\n",
        "    X_train_lstm = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced',\n",
        "                                       classes=np.unique(y_train),\n",
        "                                       y=y_train)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # Split data for validation\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        X_train_lstm, y_train, test_size=0.2, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Create tuner - Using Hyperband for efficiency on limited resources\n",
        "    tuner = kt.Hyperband(\n",
        "        build_lstm_model,\n",
        "        objective='val_accuracy',\n",
        "        max_epochs=epochs,\n",
        "        factor=3,\n",
        "        directory='lstm_tuning',\n",
        "        project_name='lstm_hyperopt'\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Search for best hyperparameters\n",
        "    tuner.search(\n",
        "        X_train_split,\n",
        "        y_train_split,\n",
        "        validation_data=(X_val_split, y_val_split),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping],\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Get best hyperparameters\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    print(\"\\nBest hyperparameters:\")\n",
        "    print(f\"LSTM units 1: {best_hps.get('lstm_units_1')}\")\n",
        "    print(f\"LSTM units 2: {best_hps.get('lstm_units_2')}\")\n",
        "    print(f\"Dense units: {best_hps.get('dense_units')}\")\n",
        "    print(f\"Dropout 1: {best_hps.get('dropout_1')}\")\n",
        "    print(f\"Dropout 2: {best_hps.get('dropout_2')}\")\n",
        "    print(f\"Dropout 3: {best_hps.get('dropout_3')}\")\n",
        "    print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "    return best_hps, tuner\n",
        "\n",
        "# Modified train_lstm function to use best hyperparameters\n",
        "def train_optimized_lstm(X_train, y_train, X_test, best_hps, n_splits=5, epochs=30, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train LSTM using the best hyperparameters found\n",
        "    \"\"\"\n",
        "    # Use TimeSeriesSplit on training data for validation\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    cv_scores = []\n",
        "\n",
        "    # Reshape data for LSTM\n",
        "    X_train_lstm = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "    X_test_lstm = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced',\n",
        "                                       classes=np.unique(y_train),\n",
        "                                       y=y_train)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # Create final model with best hyperparameters\n",
        "    model = tf.keras.Sequential([\n",
        "        Input(shape=(1, X_train.shape[1])),\n",
        "        LSTM(best_hps.get('lstm_units_1'), return_sequences=True),\n",
        "        Dropout(best_hps.get('dropout_1')),\n",
        "        LSTM(best_hps.get('lstm_units_2')),\n",
        "        Dropout(best_hps.get('dropout_2')),\n",
        "        Dense(best_hps.get('dense_units'), activation='relu'),\n",
        "        Dropout(best_hps.get('dropout_3')),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(best_hps.get('learning_rate')),\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
        "\n",
        "    # Train final model\n",
        "    history = model.fit(\n",
        "        X_train_lstm, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_proba = model.predict(X_test_lstm, verbose=0)\n",
        "\n",
        "    # Find optimal threshold\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "    y_pred = (y_pred_proba > optimal_threshold).astype(int).flatten()\n",
        "\n",
        "    return y_pred, model, cv_scores\n",
        "\n",
        "# Main execution\n",
        "print(\"Starting hyperparameter tuning...\")\n",
        "best_hps, tuner = tune_lstm_hyperparameters(X_train_filt, y_train_resampled,\n",
        "                                           max_trials=10, epochs=20)\n",
        "\n",
        "print(\"\\nTraining LSTM with optimized hyperparameters...\")\n",
        "y_pred_lstm, model_lstm, lstm_cv_scores = train_optimized_lstm(\n",
        "   X_train_filt, y_train_resampled, X_test_filt, best_hps\n",
        ")\n",
        "\n",
        "lstm_results = evaluate_model(y_test, y_pred_lstm, 'Optimized LSTM')"
      ],
      "metadata": {
        "id": "-hgxx6OTDfXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "4ug4MBM5eN85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Compare all models\n",
        "def compare_models(results_dict):\n",
        "    \"\"\"\n",
        "    Compare the performance of all models\n",
        "    \"\"\"\n",
        "    model_names = list(results_dict.keys())\n",
        "    accuracies = [results_dict[model]['accuracy'] for model in model_names]\n",
        "    improvements = [results_dict[model]['improvement'] for model in model_names]\n",
        "\n",
        "    # Create comparison chart\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Accuracy comparison\n",
        "    bars1 = ax1.bar(model_names, accuracies)\n",
        "    ax1.set_title('Model Comparison - Accuracy on 2024 Test Data')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, accuracy in zip(bars1, accuracies):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{accuracy:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # Improvement comparison\n",
        "    bars2 = ax2.bar(model_names, improvements, color=['green' if imp > 0 else 'red' for imp in improvements])\n",
        "    ax2.set_title('Model Comparison - Improvement over Naive Predictor')\n",
        "    ax2.set_ylabel('Improvement (%)')\n",
        "    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax2.axhline(y=12, color='blue', linestyle='--', linewidth=2, label='Full Credit Threshold (12%)')\n",
        "    ax2.legend()\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, improvement in zip(bars2, improvements):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5 if improvement > 0 else bar.get_height() - 1,\n",
        "                f'{improvement:.1f}%', ha='center', va='bottom' if improvement > 0 else 'top')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Compare all models\n",
        "results = {\n",
        "    'Logistic Regression': lr_results,\n",
        "    'XGBoost': xgb_results,\n",
        "    'lstm': lstm_results\n",
        "}\n",
        "\n",
        "compare_models(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "dExarGOxd6QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Print summary of results\n",
        "print(\"=\" * 50)\n",
        "print(\"MODEL PERFORMANCE SUMMARY ON 2024 TEST DATA\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, result in results.items():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"  Improvement over Naive: {result['improvement']:.2f}%\")\n",
        "    print(f\"  Precision (class 1): {result['classification_report']['1']['precision']:.4f}\")\n",
        "    print(f\"  Recall (class 1): {result['classification_report']['1']['recall']:.4f}\")\n",
        "    print(f\"  F1-score (class 1): {result['classification_report']['1']['f1-score']:.4f}\")\n",
        "\n",
        "    # Check if meets assignment requirements\n",
        "    if result['improvement'] >= 12:\n",
        "        print(f\"  STATUS: MEETS FULL CREDIT REQUIREMENT (12% improvement)\")\n",
        "    else:\n",
        "        print(f\"  STATUS: Needs {12 - result['improvement']:.2f}% more improvement for full credit\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGMm6ZXKJRo8",
        "outputId": "efd2e535-ba7f-4708-a6e4-2722ec9887dd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "MODEL PERFORMANCE SUMMARY ON 2024 TEST DATA\n",
            "==================================================\n",
            "\n",
            "Logistic Regression:\n",
            "  Accuracy: 0.6825\n",
            "  Improvement over Naive: 0.79%\n",
            "  Precision (class 1): 0.7143\n",
            "  Recall (class 1): 0.8824\n",
            "  F1-score (class 1): 0.7895\n",
            "  STATUS: Needs 11.21% more improvement for full credit\n",
            "\n",
            "XGBoost:\n",
            "  Accuracy: 0.6786\n",
            "  Improvement over Naive: 0.40%\n",
            "  Precision (class 1): 0.7306\n",
            "  Recall (class 1): 0.8294\n",
            "  F1-score (class 1): 0.7769\n",
            "  STATUS: Needs 11.60% more improvement for full credit\n",
            "\n",
            "lstm:\n",
            "  Accuracy: 0.7222\n",
            "  Improvement over Naive: 4.76%\n",
            "  Precision (class 1): 0.7358\n",
            "  Recall (class 1): 0.9176\n",
            "  F1-score (class 1): 0.8168\n",
            "  STATUS: Needs 7.24% more improvement for full credit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qk7cH-fbVM-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}